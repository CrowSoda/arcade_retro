
================================================================================
# FILE: ARCADE/src/tensorcade/__init__.py
================================================================================
"""
Tensorcade package initialization.

Exposes the worker classes at package‑level for convenience and
defines an explicit public interface.
"""

from .workers import ChunkWorker, TrainWorker, InferenceWorker   # re‑export

__all__: list[str] = [
    "ChunkWorker",
    "TrainWorker",
    "InferenceWorker",
]



================================================================================
# FILE: ARCADE/src/tensorcade/config.py
================================================================================
import os

def _find_project_root():
    candidate = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    if os.path.isdir(os.path.join(candidate, "data")):
        return candidate
    cwd = os.getcwd()
    if os.path.isdir(os.path.join(cwd, "data")):
        return cwd
    return candidate

PROJECT_ROOT = _find_project_root()
DATA_DIR      = os.path.join(PROJECT_ROOT, "data")
LOG_DIR       = os.path.join(PROJECT_ROOT, "logs")
MODELS_DIR    = os.path.join(PROJECT_ROOT, "models")

app_settings = {
    "chunk_ms":        200.0,
    "nfft":            4096,
    "noverlap":        2048,
    "dynamic_range":   80.0,
    "out_size":        1024,
    "pretrained":      False,
    "backbone":        "resnet18",
    "trainable_layers":5,
    "use_gpu":         True,
    # Optimization settings
    "batch_size":      4,
    "use_cuda_graphs": True,
    "use_pinned_memory": True,
    "prefetch_factor": 2
}


================================================================================
# FILE: ARCADE/src/tensorcade/logging_config.py
================================================================================
"""
Module for logging configuration.
"""

import os
import datetime
import logging
from .config import LOG_DIR

os.makedirs(LOG_DIR, exist_ok=True)
timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
LOG_FILENAME = os.path.join(LOG_DIR, f"tensorcade_{timestamp}.log")

logging.basicConfig(
    filename=LOG_FILENAME,
    filemode="w",
    level=logging.DEBUG,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)

logger = logging.getLogger(__name__)
logger.info("Starting TENSORCADE with requested modifications")


================================================================================
# FILE: ARCADE/src/tensorcade/utils.py
================================================================================
"""
Module for utility functions and custom widgets.
"""

from PyQt5.QtWidgets import QLabel, QDoubleSpinBox
from PyQt5.QtCore import Qt
from PyQt5.QtGui import QDoubleValidator, QValidator


def make_help_label(tooltip_text: str) -> QLabel:
    """
    Create a QLabel with a help tooltip and styling.

    :param tooltip_text: Tooltip text for the label.
    :return: Configured QLabel.
    """
    label = QLabel("?")
    label.setToolTip(tooltip_text)
    label.setAlignment(Qt.AlignCenter)
    label.setFixedSize(20, 20)
    label.setStyleSheet(
        "QLabel { color: #666; font-weight: bold; padding: 2px; }"
        "QLabel:hover { background-color: #e0f0ff; border: 1px solid "
        "#add8e6; }"
    )
    return label


class ScientificDoubleSpinBox(QDoubleSpinBox):
    """
    QDoubleSpinBox subclass for displaying values in scientific notation.
    """

    def __init__(self, parent=None):
        super().__init__(parent)
        validator = QDoubleValidator(self)
        validator.setNotation(QDoubleValidator.StandardNotation)
        self.lineEdit().setValidator(validator)

    def textFromValue(self, value):
        """
        Format the numeric value using scientific notation.
        """
        return format(value, ".6g")

    def valueFromText(self, text):
        """
        Convert text to a float value.
        """
        try:
            return float(text)
        except ValueError:
            return 0.0

    def validate(self, text, pos):
        """
        Validate the entered text.
        """
        try:
            float(text)
            return (QValidator.Acceptable, text, pos)
        except ValueError:
            return (QValidator.Invalid, text, pos)


================================================================================
# FILE: ARCADE/src/tensorcade/telemetry.py
================================================================================
import os, time, socket, logging, contextlib, sys
from typing import Optional, Dict, Tuple

import torch
from prometheus_client import (
    CollectorRegistry, Gauge, Summary, Histogram,
    pushadd_to_gateway, delete_from_gateway,
)

HOST   = socket.gethostname()
PID    = str(os.getpid())
PUSHGW = os.getenv("PUSHGATEWAY_URL", "http://192.168.20.52:30091/")

if not logging.getLogger("tensorcade.telemetry").handlers:
    logging.basicConfig(level=logging.INFO,
                        format="%(asctime)s %(levelname)s %(message)s",
                        stream=sys.stderr)

try:
    import psutil
except ImportError:
    psutil = None

try:
    import pyRAPL
    pyRAPL.setup()
    RAPL_OK = True
except Exception:
    RAPL_OK = False

try:
    import pynvml
    pynvml.nvmlInit()
    NVML_OK = True
except Exception:
    NVML_OK = False

REGISTRY: CollectorRegistry = CollectorRegistry()
_SUMMARY : Dict[Tuple[str, Tuple[str, ...]], Summary]   = {}
_HIST    : Dict[Tuple[str, Tuple[str, ...]], Histogram] = {}
_GAUGE   : Dict[Tuple[str, Tuple[str, ...]], Gauge]     = {}

BUCKETS = (.002,.005,.01,.025,.05,.075,.1,.15,.2,.3,.4,.5,.75,1,1.5,2,4,8)

def _hist(n,h,k,b=BUCKETS):
    t=(n,k)
    if t not in _HIST:
        _HIST[t]=Histogram(n,h,k,registry=REGISTRY,buckets=b)
    return _HIST[t]

def _summ(n,h,k):
    t=(n,k)
    if t not in _SUMMARY:
        _SUMMARY[t]=Summary(n,h,k,registry=REGISTRY)
    return _SUMMARY[t]

def _gauge(n,h,k):
    t=(n,k)
    if t not in _GAUGE:
        _GAUGE[t]=Gauge(n,h,k,registry=REGISTRY)
    return _GAUGE[t]

def _push():
    pushadd_to_gateway(PUSHGW, job="tensorcade", registry=REGISTRY)

def _safe_push(_: CollectorRegistry):
    _push()

def _cleanup():
    try: delete_from_gateway(PUSHGW, job="tensorcade")
    except Exception as e: logging.getLogger("tensorcade.telemetry").warning("cleanup failed: %s",e)

def _cpu_power() -> Optional[float]:
    if not RAPL_OK: return None
    dt=0.05
    m=pyRAPL.Measurement("")
    m.begin(); time.sleep(dt); m.end()
    if not m.result.pkg: return None
    return sum(m.result.pkg)/1e6/dt

def _gpu_stats() -> Tuple[Optional[int],Optional[int],Optional[int],Optional[float]]:
    if not NVML_OK: return None,None,None,None
    try:
        h=pynvml.nvmlDeviceGetHandleByIndex(0)
        vram=pynvml.nvmlDeviceGetMemoryInfo(h).used
        util=pynvml.nvmlDeviceGetUtilizationRates(h).gpu
        try: fan=pynvml.nvmlDeviceGetFanSpeed(h)
        except pynvml.NVMLError_NotSupported: fan=None
        power=pynvml.nvmlDeviceGetPowerUsage(h)/1_000.0
        return vram,util,fan,power
    except Exception: return None,None,None,None

def _collect(t0:float,mode:str,
             model:str,backend:str,device:str,
             ds_size:Optional[int],extra:Optional[Dict[str,str]])->None:

    dur=time.time()-t0
    lbl=dict(host=HOST,pid=PID,mode=mode,
             model_name=model,backend=backend,device_id=device)
    if extra: lbl.update(extra)
    keys=tuple(lbl.keys())

    if mode=="inference":
        _hist("tensorcade_inference_duration_seconds","",keys).labels(**lbl).observe(dur)
    else:
        _summ("tensorcade_training_duration_seconds","",keys).labels(**lbl).observe(dur)

    rss=cpu_log=cpu_phy=core_usage=None
    if psutil:
        p=psutil.Process()
        rss=p.memory_info().rss
        core_usage=psutil.cpu_percent(percpu=True)
        try: cpu_log=len(p.cpu_affinity())
        except AttributeError: cpu_log=psutil.cpu_count(True) or 1
        tot_log=psutil.cpu_count(True) or cpu_log
        tot_phy=psutil.cpu_count(False) or tot_log
        cpu_phy=cpu_log*(tot_phy/tot_log)

    vram_alloc=vram_reserved=None
    try:
        vram_alloc=torch.cuda.memory_allocated()
        vram_reserved=torch.cuda.memory_reserved()
    except Exception: pass

    vram_drv,gpu_util,gpu_fan,gpu_pow=_gpu_stats()
    cpu_pow=_cpu_power()

    def g(n,v):
        if v is not None:
            _gauge(f"tensorcade_{mode}_{n}","",keys).labels(**lbl).set(v)

    g("cpu_cores_logical",cpu_log)
    g("cpu_cores_physical",cpu_phy)
    g("ram_bytes",rss)
    g("vram_bytes",vram_alloc if vram_alloc else vram_drv)
    g("vram_reserved_bytes",vram_reserved)
    g("gpu_utilization",gpu_util)
    g("gpu_fan_speed",gpu_fan)
    g("power_watts_gpu",gpu_pow)
    g("power_watts_cpu",cpu_pow)

    if core_usage is not None:
        for idx,pct in enumerate(core_usage):
            k2=keys+("core",)
            _gauge(f"tensorcade_{mode}_cpu_core_usage_percent","",k2).labels(**lbl,core=str(idx)).set(pct)

    if mode=="training" and ds_size is not None:
        _gauge("tensorcade_training_dataset_size","",keys).labels(**lbl).set(ds_size)

    _push()

class _Span(contextlib.AbstractContextManager):
    def __init__(self,model,backend,device,ds,extra):
        self.model,self.backend,self.device=model,backend,device
        self.ds,self.extra=ds,extra or {}
    def __enter__(self): self._t0=time.time(); return self

class inference_span(_Span):
    def __exit__(self,*_):
        _collect(self._t0,"inference",self.model,
                 self.backend,self.device,None,self.extra)

class training_epoch_span(_Span):
    def __exit__(self,*_):
        _collect(self._t0,"training",self.model,
                 self.backend,self.device,self.ds,self.extra)

def push_inference_timings(t_pre,t_xfer,t_fwd,t_map,t_emit,
                           model,backend,device,extra):
    lbl=dict(host=HOST,pid=PID,mode="inference",
             model_name=model,backend=backend,device_id=device)
    if extra: lbl.update(extra)
    keys=tuple(lbl.keys())
    _summ("tensorcade_inference_preprocess_duration_seconds","",keys).labels(**lbl).observe(t_pre)
    _summ("tensorcade_inference_tensor_transfer_duration_seconds","",keys).labels(**lbl).observe(t_xfer)
    _summ("tensorcade_inference_forward_duration_seconds","",keys).labels(**lbl).observe(t_fwd)
    _summ("tensorcade_inference_pixel2freq_duration_seconds","",keys).labels(**lbl).observe(t_map)
    _summ("tensorcade_inference_emit_duration_seconds","",keys).labels(**lbl).observe(t_emit)
    _push()

__all__=[
    "inference_span","training_epoch_span",
    "push_inference_timings","_safe_push","_cleanup",
    "HOST","PID"
]


================================================================================
# FILE: ARCADE/src/tensorcade/workers.py
================================================================================
# tensorcade/workers.py
"""
Module for worker classes handling background tasks.
"""

import sys
import os
import json
import time
import numpy as np
import traceback
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Tuple

import torch
import torch.nn.functional as F
import torchvision
from PyQt5.QtCore import QObject, pyqtSignal
from prometheus_client import CollectorRegistry, Gauge
from torchvision.models import ResNet18_Weights, ResNet50_Weights
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor

from .config import app_settings, DATA_DIR, MODELS_DIR
from .logging_config import logger
from .telemetry import (
    HOST,
    _safe_push,
    inference_span,
    push_inference_timings,
    training_epoch_span,
)

__all__ = ["ChunkWorker", "TrainWorker", "InferenceWorker"]

# --------------------------------------------------------------------------- #
#  Helpers used by the inference path (legacy sections below are untouched)  #
# --------------------------------------------------------------------------- #
_DEVICE = torch.device(
    "cuda" if torch.cuda.is_available() and app_settings["use_gpu"] else "cpu"
)
_NFFT, _HOP = 1024, 256
_WINDOW = torch.hann_window(_NFFT, device=_DEVICE)


def _weights_for_backbone(name: str):
    if name == "resnet50":
        return ResNet50_Weights.DEFAULT if app_settings["pretrained"] else None
    return ResNet18_Weights.DEFAULT if app_settings["pretrained"] else None


def _build_detector(num_classes: int):
    if app_settings["backbone"] == "resnet50":
        w = _weights_for_backbone("resnet50")
        m = torchvision.models.detection.fasterrcnn_resnet50_fpn(
            weights=w, weights_backbone=w
        )
        m.roi_heads.box_predictor = FastRCNNPredictor(
            m.roi_heads.box_predictor.cls_score.in_features, num_classes
        )
        return m
    w = _weights_for_backbone("resnet18")
    bb = torchvision.models.detection.backbone_utils.resnet_fpn_backbone(
        "resnet18", weights=w, trainable_layers=app_settings["trainable_layers"]
    )
    return torchvision.models.detection.FasterRCNN(bb, num_classes=num_classes)


# --------------------------------------------------------------------------- #
#  Legacy ChunkWorker (verbatim)                                              #
# --------------------------------------------------------------------------- #
class ChunkWorker(QObject):
    """
    Worker to process IQ data chunks and generate spectrogram images.
    Only saves images for chunks where labels are present.
    """

    progress = pyqtSignal(str)
    finished = pyqtSignal(bool)
    progressPercent = pyqtSignal(int)

    def __init__(self, input_dir):
        super().__init__()
        self.input_dir = input_dir
        self._stop_flag = False

    def stop(self):
        self._stop_flag = True
        logger.info("ChunkWorker stop requested.")

    def run(self):
        try:
            import cv2
            from scipy.signal import stft
            import matplotlib.pyplot as plt

            chunk_ms = app_settings["chunk_ms"]
            nfft = app_settings["nfft"]
            noverlap = app_settings["noverlap"]
            dynamic_range = app_settings["dynamic_range"]
            out_size = app_settings["out_size"]

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            base_output_dir = os.path.join(DATA_DIR, "spec_image")
            out_dir = os.path.join(base_output_dir, timestamp)
            os.makedirs(out_dir, exist_ok=True)
            logger.info(f"Output directory for chunks: {out_dir}")

            data_files = [
                f for f in os.listdir(self.input_dir) if f.endswith(".sigmf-data")
            ]
            if not data_files:
                self.progress.emit(f"[ERROR] No .sigmf-data in {self.input_dir}")
                self.finished.emit(False)
                return

            total_chunks = 0
            meta_info = []
            for df in data_files:
                prefix = df[:-11]
                meta_path = os.path.join(self.input_dir, prefix + ".sigmf-meta")
                if not os.path.isfile(meta_path):
                    continue
                with open(meta_path, "r") as f:
                    meta = json.load(f)
                sr = meta.get("global", {}).get("core:sample_rate", 1e6)
                data_path = os.path.join(self.input_dir, df)
                fs = os.path.getsize(data_path)
                total_samps = fs // 8
                chunk_samps = int(sr * (chunk_ms / 1000.0))
                n_chunks = (total_samps + chunk_samps - 1) // chunk_samps
                total_chunks += n_chunks
                meta_info.append((df, prefix, sr, total_samps, chunk_samps))

            if total_chunks == 0:
                self.progress.emit(
                    "[ERROR] No chunks found. Check data or parameters."
                )
                self.finished.emit(False)
                return

            processed_chunks = 0
            for (datafile, prefix, sr, total_samps, chunk_samps) in meta_info:
                if self._stop_flag:
                    self.progress.emit("[INFO] Stopped by user.")
                    self.finished.emit(False)
                    return

                self.progress.emit(f"[INFO] Working on I/Q file {datafile}")

                data_path = os.path.join(self.input_dir, datafile)
                labels_path = os.path.join(self.input_dir, prefix + "_labels.json")
                signals = []
                if os.path.isfile(labels_path):
                    with open(labels_path, "r") as f:
                        lb = json.load(f)
                    signals = lb.get("signals", [])

                with open(data_path, "rb") as f:
                    raw = f.read()
                iq_data = np.frombuffer(raw, dtype=np.complex64)
                n_chunks = (iq_data.size + chunk_samps - 1) // chunk_samps

                def box_to_chunk_coords(sig, cstart, cend, tb, fb, freqs):
                    t_s = sig["time_start"]
                    t_e = sig["time_stop"]
                    if t_e < cstart or t_s > cend:
                        return None
                    is_tstart = max(t_s, cstart)
                    is_tstop = min(t_e, cend)
                    if is_tstop <= is_tstart:
                        return None
                    chunk_len = cend - cstart
                    local_t_s = is_tstart - cstart
                    local_t_e = is_tstop - cstart
                    x_min = (local_t_s / chunk_len) * tb
                    x_max = (local_t_e / chunk_len) * tb

                    f_l = sig["freq_low"] * 1e6
                    f_h = sig["freq_high"] * 1e6
                    fmn = freqs[0]
                    fmx = freqs[-1]
                    if f_h < fmn or f_l > fmx:
                        return None
                    f_h = min(f_h, fmx)
                    f_l = max(f_l, fmn)
                    if f_l >= f_h:
                        return None

                    def find_bin_for_freq(ff):
                        i = np.searchsorted(freqs, ff, side="left")
                        return max(0, min(i, len(freqs) - 1))

                    y_min_idx = find_bin_for_freq(f_l)
                    y_max_idx = find_bin_for_freq(f_h)
                    if y_max_idx <= y_min_idx:
                        return None
                    return {
                        "label": sig.get("label", "signal"),
                        "x_min": float(x_min),
                        "x_max": float(x_max),
                        "y_min": float(y_min_idx),
                        "y_max": float(y_max_idx),
                    }

                for chunk_idx in range(n_chunks):
                    if self._stop_flag:
                        self.progress.emit("[INFO] Stopped by user.")
                        self.finished.emit(False)
                        return
                    start_samp = chunk_idx * chunk_samps
                    if start_samp >= iq_data.size:
                        break
                    end_samp = min(start_samp + chunk_samps, iq_data.size)
                    chunk_data = iq_data[start_samp:end_samp]
                    chunk_start_s = start_samp / sr
                    chunk_end_s = end_samp / sr

                    f_vals, t_vals, Zxx = stft(
                        chunk_data,
                        fs=sr,
                        nperseg=nfft,
                        noverlap=min(noverlap, nfft - 1),
                        return_onesided=False,
                    )
                    f_vals_shifted = np.fft.fftshift(f_vals)
                    Zxx_shift = np.fft.fftshift(Zxx, axes=0)
                    sxx = np.abs(Zxx_shift) ** 2
                    sxx_db = 10 * np.log10(sxx + 1e-12)
                    vmax = sxx_db.max()
                    vmin = vmax - dynamic_range
                    sxx_clamped = np.clip(sxx_db, vmin, vmax)
                    sxx_db_float = sxx_clamped.astype(np.float32)

                    resized = cv2.resize(
                        sxx_db_float, (out_size, out_size), interpolation=cv2.INTER_AREA
                    )
                    resized = np.flipud(resized)

                    tile_bboxes = []
                    for sig in signals:
                        bc = box_to_chunk_coords(
                            sig,
                            chunk_start_s,
                            chunk_end_s,
                            sxx_db_float.shape[1],
                            sxx_db_float.shape[0],
                            f_vals_shifted,
                        )
                        if bc is None:
                            continue
                        scale_x = out_size / float(sxx_db_float.shape[1])
                        scale_y = out_size / float(sxx_db_float.shape[0])
                        x1 = bc["x_min"] * scale_x
                        x2 = bc["x_max"] * scale_x
                        y1 = bc["y_min"] * scale_y
                        y2 = bc["y_max"] * scale_y
                        if x2 <= x1 or y2 <= y1:
                            continue
                        tile_bboxes.append(
                            {
                                "label": bc["label"],
                                "x_min": float(x1),
                                "y_min": float(y1),
                                "x_max": float(x2),
                                "y_max": float(y2),
                            }
                        )

                    if tile_bboxes:
                        out_png = os.path.join(
                            out_dir, f"chunk_{prefix}_{chunk_idx:03d}.png"
                        )
                        plt.imsave(out_png, resized, cmap="gray", origin="lower")

                        out_json = os.path.join(
                            out_dir, f"chunk_{prefix}_{chunk_idx:03d}_bboxes.json"
                        )
                        with open(out_json, "w") as jf:
                            json.dump(
                                {
                                    "image": os.path.basename(out_png),
                                    "width": out_size,
                                    "height": out_size,
                                    "bboxes": tile_bboxes,
                                },
                                jf,
                                indent=2,
                            )
                        self.progress.emit(
                            f"[INFO] Saved labeled chunk {chunk_idx} for file {datafile}"
                        )
                    else:
                        self.progress.emit(
                            f"[INFO] Skipped unlabeled chunk {chunk_idx} for file {datafile}"
                        )

                    processed_chunks += 1
                    pct = int((processed_chunks / total_chunks) * 100)
                    self.progressPercent.emit(pct)

            self.progress.emit("[INFO] Chunk creation complete.")
            self.finished.emit(True)

        except Exception:
            err = "".join(traceback.format_exception(*sys.exc_info()))
            self.progress.emit(f"[ERROR] {err}")
            self.finished.emit(False)


# --------------------------------------------------------------------------- #
#  Legacy TrainWorker (verbatim)                                              #
# --------------------------------------------------------------------------- #
class TrainWorker(QObject):
    """
    Worker to train the detection model in background.
    """

    progress = pyqtSignal(str)
    finished = pyqtSignal(bool)
    progressPercent = pyqtSignal(int)

    def __init__(self, data_dir, train_params):
        super().__init__()
        self.data_dir = data_dir
        self._train_params = train_params
        self._stop_flag = False

    def stop(self):
        self._stop_flag = True
        logger.info("TrainWorker stop requested.")

    def run(self):
        try:
            from PIL import Image
            import torchvision
            from torchvision import transforms as T
            from torch.utils.data import Dataset, DataLoader
            from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
            from torchvision.models.detection.backbone_utils import (
                resnet_fpn_backbone,
            )
            from sklearn.model_selection import KFold
            import torch
            import numpy as np

            backbone = app_settings["backbone"]
            pretrained = app_settings["pretrained"]
            trainable_layers = app_settings["trainable_layers"]

            ep = self._train_params["epochs"]
            bs = self._train_params["batch_size"]
            lr = self._train_params["lr"]
            k_folds = self._train_params["k_folds"]
            out_model = self._train_params["out_model"]
            early_stop_patience = self._train_params.get("early_stop_patience", 10)

            if not out_model.startswith(MODELS_DIR):
                base = os.path.basename(out_model)
                out_model = os.path.join(MODELS_DIR, base)
                self._train_params["out_model"] = out_model

            class DetectionDataset(Dataset):
                def __init__(self, data_dir, augment=False):
                    super().__init__()
                    self.data_dir = data_dir
                    self.augment = augment
                    self.samples = []
                    self.label_map = {"background": 0}
                    for fn in os.listdir(data_dir):
                        if fn.endswith("_bboxes.json"):
                            jpath = os.path.join(data_dir, fn)
                            with open(jpath, "r") as f:
                                info = json.load(f)
                            img_name = info.get("image", "")
                            if not img_name:
                                continue
                            img_path = os.path.join(data_dir, img_name)
                            if not os.path.isfile(img_path):
                                continue
                            for b in info.get("bboxes", []):
                                lbl = b.get("label", "signal")
                                if lbl not in self.label_map:
                                    self.label_map[lbl] = len(self.label_map)
                            self.samples.append((img_path, info))
                    self.num_classes = len(self.label_map)
                    self.transform = T.ToTensor()

                def __len__(self):
                    return len(self.samples)

                def __getitem__(self, idx):
                    import torch
                    import numpy as _np

                    img_path, info = self.samples[idx]
                    img = Image.open(img_path).convert("RGB")
                    bbs = info.get("bboxes", [])
                    boxes, labels = [], []
                    for b in bbs:
                        boxes.append(
                            [b["x_min"], b["y_min"], b["x_max"], b["y_max"]]
                        )
                        labels.append(self.label_map[b.get("label", "signal")])
                    if not boxes:
                        boxes = _np.zeros((0, 4), dtype=_np.float32)
                        labels = _np.zeros((0,), dtype=_np.int64)
                    boxes_t = torch.as_tensor(boxes, dtype=torch.float32)
                    labels_t = torch.as_tensor(labels, dtype=torch.int64)
                    target = {
                        "boxes": boxes_t,
                        "labels": labels_t,
                        "image_id": torch.tensor([idx], dtype=torch.int64),
                    }
                    img_t = self.transform(img)
                    return img_t, target

            def build_model(num_classes, backbone, pretrained, trainable_layers):
                import torchvision.models.detection as _det
                from torchvision.models.detection.faster_rcnn import (
                    FastRCNNPredictor,
                )
                from torchvision.models.detection.backbone_utils import (
                    resnet_fpn_backbone as _bb,
                )

                if backbone == "resnet50":
                    m = _det.fasterrcnn_resnet50_fpn(
                        weights="DEFAULT" if pretrained else None
                    )
                    inf = m.roi_heads.box_predictor.cls_score.in_features
                    m.roi_heads.box_predictor = FastRCNNPredictor(inf, num_classes)
                    return m
                else:
                    bb = _bb(
                        "resnet18",
                        pretrained=pretrained,
                        trainable_layers=trainable_layers,
                    )
                    return _det.FasterRCNN(bb, num_classes=num_classes)

            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            ds_full = DetectionDataset(self.data_dir, augment=False)
            if len(ds_full) == 0:
                self.progress.emit("[ERROR] No data found in dataset.")
                self.finished.emit(False)
                return

            def collate_fn(x):
                return list(zip(*x))

            if k_folds <= 1:
                import numpy as _np

                n_total = len(ds_full)
                idxs = _np.arange(n_total)
                _np.random.shuffle(idxs)
                split = int(0.8 * n_total)
                train_idx = idxs[:split]
                val_idx = idxs[split:]

                ds_train = DetectionDataset(self.data_dir, augment=True)
                ds_val = DetectionDataset(self.data_dir, augment=False)
                ds_train.label_map = ds_full.label_map
                ds_val.label_map = ds_full.label_map
                ds_train.samples = [ds_full.samples[i] for i in train_idx]
                ds_val.samples = [ds_full.samples[i] for i in val_idx]

                train_loader = DataLoader(
                    ds_train, batch_size=bs, shuffle=True, collate_fn=collate_fn
                )
                val_loader = DataLoader(
                    ds_val, batch_size=bs, shuffle=False, collate_fn=collate_fn
                )

                model = build_model(
                    ds_full.num_classes, backbone, pretrained, trainable_layers
                )
                model.to(device)
                optimizer = torch.optim.Adam(
                    [p for p in model.parameters() if p.requires_grad], lr=lr
                )
                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                    optimizer,
                    mode="max",
                    factor=0.1,
                    patience=5,
                    verbose=True,
                    min_lr=1e-6,
                )

                total_steps = ep * len(train_loader)
                current_step = 0
                best_f1 = 0.0
                epochs_without_improve = 0

                for epoch in range(ep):
                    if self._stop_flag:
                        self.progress.emit("[INFO] Stopped by user.")
                        self.finished.emit(False)
                        return
                    model.train()
                    epoch_loss = 0.0
                    for imgs, tgts in train_loader:
                        if self._stop_flag:
                            self.progress.emit("[INFO] Stopped by user.")
                            self.finished.emit(False)
                            return
                        imgs_gpu = [i.to(device) for i in imgs]
                        tgts_gpu = [
                            {k: v.to(device) for k, v in t.items()} for t in tgts
                        ]
                        loss_dict = model(imgs_gpu, tgts_gpu)
                        loss = sum(loss for loss in loss_dict.values())
                        optimizer.zero_grad()
                        loss.backward()
                        optimizer.step()
                        epoch_loss += loss.item()
                        current_step += 1
                        self.progressPercent.emit(
                            int((current_step / total_steps) * 100)
                        )

                    self.progress.emit(
                        f"[Epoch {epoch+1}/{ep}] Train Loss={epoch_loss/len(train_loader):.4f}"
                    )

                    model.eval()
                    with torch.inference_mode():
                        tot_tp = tot_fp = tot_fn = 0
                        for imgs, tgts in val_loader:
                            imgs_gpu = [i.to(device) for i in imgs]
                            outs = model(imgs_gpu)
                            for out, tgt in zip(outs, tgts):
                                gt_boxes = tgt["boxes"].cpu().numpy()
                                pb = out["boxes"].cpu().numpy()
                                sc = out["scores"].cpu().numpy()
                                keep = sc >= 0.5
                                pb = pb[keep]
                                tp = fp = 0
                                for pbox in pb:
                                    found = False
                                    for gbox in gt_boxes:
                                        xA = max(pbox[0], gbox[0])
                                        yA = max(pbox[1], gbox[1])
                                        xB = min(pbox[2], gbox[2])
                                        yB = min(pbox[3], gbox[3])
                                        interW = max(0, xB - xA)
                                        interH = max(0, yB - yA)
                                        interA = interW * interH
                                        area_box = (
                                            (pbox[2] - pbox[0])
                                            * (pbox[3] - pbox[1])
                                        )
                                        area_gt = (
                                            (gbox[2] - gbox[0])
                                            * (gbox[3] - gbox[1])
                                        )
                                        union = area_box + area_gt - interA
                                        iou = interA / union if union > 1e-12 else 0
                                        if iou >= 0.5:
                                            found = True
                                            break
                                    if found:
                                        tp += 1
                                    else:
                                        fp += 1
                                fn = len(gt_boxes) - tp
                                tot_tp += tp
                                tot_fp += fp
                                tot_fn += fn
                        prec = tot_tp / (tot_tp + tot_fp + 1e-12)
                        rec = tot_tp / (tot_tp + tot_fn + 1e-12)
                        f1 = (
                            2 * prec * rec / (prec + rec)
                            if (prec + rec) > 0
                            else 0
                        )
                    self.progress.emit(
                        f"  Val => Prec={prec:.3f}, Rec={rec:.3f}, F1={f1:.3f}"
                    )

                    if f1 > best_f1:
                        best_f1 = f1
                        epochs_without_improve = 0
                    else:
                        epochs_without_improve += 1

                    if epochs_without_improve >= early_stop_patience:
                        self.progress.emit(
                            f"[INFO] Early stopping at epoch {epoch+1}, best F1={best_f1:.3f}"
                        )
                        break
                    scheduler.step(f1)

                os.makedirs(os.path.dirname(out_model), exist_ok=True)
                torch.save(model.state_dict(), out_model)
                self.progress.emit(f"[INFO] Model saved => {out_model}")
                self.finished.emit(True)

            else:
                from sklearn.model_selection import KFold as _KF
                import numpy as _np

                idxs = _np.arange(len(ds_full))
                _np.random.shuffle(idxs)
                kf = _KF(n_splits=k_folds, shuffle=False)
                all_metrics = []
                step_count = 0
                total_steps = k_folds * ep

                for fold_id, (train_idx, val_idx) in enumerate(
                    kf.split(idxs), start=1
                ):
                    ds_train = DetectionDataset(self.data_dir, augment=True)
                    ds_val = DetectionDataset(self.data_dir, augment=False)
                    ds_train.label_map = ds_full.label_map
                    ds_val.label_map = ds_full.label_map
                    ds_train.samples = [ds_full.samples[i] for i in train_idx]
                    ds_val.samples = [ds_full.samples[i] for i in val_idx]

                    train_loader = DataLoader(
                        ds_train,
                        batch_size=bs,
                        shuffle=True,
                        collate_fn=collate_fn,
                    )
                    val_loader = DataLoader(
                        ds_val,
                        batch_size=bs,
                        shuffle=False,
                        collate_fn=collate_fn,
                    )

                    model = build_model(
                        ds_full.num_classes, backbone, pretrained, trainable_layers
                    )
                    model.to(device)
                    optimizer = torch.optim.Adam(
                        [p for p in model.parameters() if p.requires_grad], lr=lr
                    )
                    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                        optimizer,
                        mode="max",
                        factor=0.1,
                        patience=5,
                        verbose=True,
                        min_lr=1e-6,
                    )

                    best_fold_f1 = 0.0
                    epochs_no_improve_fold = 0

                    for epoch in range(ep):
                        if self._stop_flag:
                            self.progress.emit("[INFO] Stopped by user.")
                            self.finished.emit(False)
                            return
                        model.train()
                        fold_loss = 0.0
                        for imgs, tgts in train_loader:
                            if self._stop_flag:
                                self.progress.emit("[INFO] Stopped by user.")
                                self.finished.emit(False)
                                return
                            imgs_gpu = [i.to(device) for i in imgs]
                            tgts_gpu = [
                                {k: v.to(device) for k, v in t.items()}
                                for t in tgts
                            ]
                            loss_dict = model(imgs_gpu, tgts_gpu)
                            loss = sum(loss for loss in loss_dict.values())
                            optimizer.zero_grad()
                            loss.backward()
                            optimizer.step()
                            fold_loss += loss.item()
                        self.progress.emit(
                            f"[Fold {fold_id} E{epoch+1}/{ep}] Train Loss={fold_loss/len(train_loader):.4f}"
                        )
                        step_count += 1
                        self.progressPercent.emit(
                            int((step_count / total_steps) * 100)
                        )

                        model.eval()
                        with torch.inference_mode():
                            tp_f = fp_f = fn_f = 0
                            for imgs, tgts in val_loader:
                                imgs_gpu = [i.to(device) for i in imgs]
                                outs = model(imgs_gpu)
                                for out, tgt in zip(outs, tgts):
                                    gt_boxes = tgt["boxes"].cpu().numpy()
                                    pb = out["boxes"].cpu().numpy()
                                    sc = out["scores"].cpu().numpy()
                                    keep = sc >= 0.5
                                    pb = pb[keep]
                                    tp_i = fp_i = 0
                                    for pbox in pb:
                                        found = False
                                        for gbox in gt_boxes:
                                            xA = max(pbox[0], gbox[0])
                                            yA = max(pbox[1], gbox[1])
                                            xB = min(pbox[2], gbox[2])
                                            yB = min(pbox[3], gbox[3])
                                            interW = max(0, xB - xA)
                                            interH = max(0, yB - yA)
                                            interA = interW * interH
                                            area = (
                                                (pbox[2] - pbox[0])
                                                * (pbox[3] - pbox[1])
                                                + (gbox[2] - gbox[0])
                                                * (gbox[3] - gbox[1])
                                                - interA
                                            )
                                            iou = (
                                                interA / area if area > 1e-12 else 0
                                            )
                                            if iou >= 0.5:
                                                found = True
                                                break
                                        if found:
                                            tp_i += 1
                                        else:
                                            fp_i += 1
                                    fn_i = len(gt_boxes) - tp_i
                                    tp_f += tp_i
                                    fp_f += fp_i
                                    fn_f += fn_i
                            prec_f = tp_f / (tp_f + fp_f + 1e-12)
                            rec_f = tp_f / (tp_f + fn_f + 1e-12)
                            f1_f = (
                                2 * prec_f * rec_f / (prec_f + rec_f)
                                if (prec_f + rec_f) > 0
                                else 0
                            )
                        self.progress.emit(
                            f"[CV Fold {fold_id}] Prec={prec_f:.3f}, Rec={rec_f:.3f}, F1={f1_f:.3f}"
                        )

                        if f1_f > best_fold_f1:
                            best_fold_f1 = f1_f
                            epochs_no_improve_fold = 0
                        else:
                            epochs_no_improve_fold += 1

                        if epochs_no_improve_fold >= early_stop_patience:
                            self.progress.emit(
                                f"[CV Fold {fold_id}] Early stopping at E{epoch+1}, best F1={best_fold_f1:.3f}"
                            )
                            break
                        scheduler.step(f1_f)

                    fold_out = out_model.replace(
                        ".pth", f"_fold{fold_id}.pth"
                    )
                    os.makedirs(os.path.dirname(fold_out), exist_ok=True)
                    torch.save(model.state_dict(), fold_out)
                    self.progress.emit(f"[CV] Fold saved => {fold_out}")
                    all_metrics.append((prec_f, rec_f, f1_f))

                if all_metrics:
                    arr = np.array(all_metrics)
                    mp, mr, mf = (
                        arr[:, 0].mean(),
                        arr[:, 1].mean(),
                        arr[:, 2].mean(),
                    )
                    self.progress.emit(
                        f"[CV All] Prec={mp:.3f}, Rec={mr:.3f}, F1={mf:.3f}"
                    )

                self.finished.emit(True)

        except Exception:
            err = "".join(traceback.format_exception(*sys.exc_info()))
            self.progress.emit(f"[ERROR] {err}")
            self.finished.emit(False)


# --------------------------------------------------------------------------- #
#  InferenceWorker – kept exactly as in the new branch (untouched)            #
# --------------------------------------------------------------------------- #
class InferenceWorker(QObject):
    """
    Worker for performing model inference (streaming by chunk).
    """

    spectrogramReady = pyqtSignal(np.ndarray, list, tuple)
    progress = pyqtSignal(str)
    finished = pyqtSignal(bool)
    detectionJson = pyqtSignal(dict)
    inferenceSpeedUpdated = pyqtSignal(float)
    throughputUpdated = pyqtSignal(float)  # chunks per second
    totalTimeUpdated = pyqtSignal(float)  # total time per chunk

    def __init__(self, params):
        super().__init__()
        self.params = params
        self._stop_flag = False
        self._pause_flag = False
        self._display_speed_percent = 100  # Default to full speed
        self.model = None
        self.device = None
        self.file = None
        self.chunk_count = 0
        self.flowgraph = None
        self.single_pass_dir = None
        # Batch processing attributes
        self.batch_size = app_settings.get("batch_size", 4)
        self.chunk_buffer = []
        self.batch_process_interval = 0.2  # seconds
        self.last_batch_time = time.time()
        # Throughput tracking - will be initialized in run()
        self.start_time = None
        self.throughput_update_interval = 1.0  # Update every second
        self.last_throughput_update = None

    def pause(self):
        """Pause the inference processing"""
        self._pause_flag = True
        logger.info("InferenceWorker pause requested.")

    def resume(self):
        """Resume the inference processing"""
        self._pause_flag = False
        logger.info("InferenceWorker resume requested.")

    def setDisplaySpeed(self, speed_percent):
        """Set the display speed as a percentage (1-100)"""
        self._display_speed_percent = max(1, min(100, speed_percent))
        logger.info(f"InferenceWorker display speed set to {self._display_speed_percent}%.")

    def stop(self):
        self._stop_flag = True
        if self.flowgraph:
            self.flowgraph.stop()
        if self.file:
            try:
                self.file.close()
            except Exception:
                pass
        logger.info("InferenceWorker stop requested.")

    def run(self):
        try:
            import torch
            import torch.backends.cudnn as cudnn

            cudnn.benchmark = True

            import torchvision
            from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
            from torchvision.models.detection.backbone_utils import \
                resnet_fpn_backbone

            backbone = app_settings["backbone"]
            pretrained = app_settings["pretrained"]
            trainable_layers = app_settings["trainable_layers"]
            nfft = app_settings["nfft"]
            noverlap = app_settings["noverlap"]
            dynamic_range = app_settings["dynamic_range"]
            out_size = app_settings["out_size"]
            use_gpu = app_settings["use_gpu"]
            chunk_ms = app_settings["chunk_ms"]

            mode_offline = self.params["offline_mode"]
            sample_rate = self.params["sample_rate"]
            bandwidth = self.params["bandwidth"]
            file_path = self.params["file_path"]
            model_path = self.params["model_path"]
            num_classes = self.params["num_classes"]
            precision = self.params.get("precision", "fp32")
            single_pass = self.params.get("single_pass", False)

            def build_frcnn(num_classes, backbone, pretrained, trainable_layers):
                if backbone == "resnet50":
                    m = torchvision.models.detection.fasterrcnn_resnet50_fpn(
                        weights="DEFAULT" if pretrained else None
                    )
                    inf = (
                        m.roi_heads.box_predictor.cls_score.in_features
                    )
                    m.roi_heads.box_predictor = FastRCNNPredictor(inf, num_classes)
                    return m
                else:
                    bb = resnet_fpn_backbone(
                        "resnet18",
                        pretrained=pretrained,
                        trainable_layers=trainable_layers,
                    )
                    return torchvision.models.detection.FasterRCNN(
                        bb, num_classes=num_classes
                    )

            self.device = torch.device(
                "cuda" if (torch.cuda.is_available() and use_gpu) else "cpu"
            )
            self.progress.emit(f"[INFO] Using device => {self.device}")
            self.model = build_frcnn(
                num_classes, backbone, pretrained, trainable_layers
            )
            ckpt = torch.load(model_path, map_location=self.device)
            self.model.load_state_dict(ckpt)
            self.model.to(self.device)
            self.model.eval()
            self.progress.emit(f"[INFO] Model loaded => {model_path}")

            if precision == "fp16":
                self.model.half()
            
            # Apply torch.compile for optimization (PyTorch 2.0+)
            # Note: Detection models have dynamic shapes that don't work well with default compile
            if hasattr(torch, 'compile') and False:  # Disabled for detection models
                try:
                    self.progress.emit("[INFO] Compiling model with torch.compile...")
                    # For detection models, use max-autotune mode without CUDA graphs
                    self.model = torch.compile(
                        self.model, 
                        mode="max-autotune",
                        disable=True  # Disable CUDA graphs for dynamic models
                    )
                    self.progress.emit("[INFO] Model compilation complete!")
                except Exception as e:
                    self.progress.emit(f"[WARNING] torch.compile failed: {e}, using uncompiled model")

            self.chunk_size = int(sample_rate * (chunk_ms / 1000.0))
            
            # Initialize throughput tracking now that inference is starting
            self.start_time = time.time()
            self.last_throughput_update = time.time()

            if not mode_offline:
                try:
                    import gnuradio  # noqa
                except ImportError:
                    self.progress.emit(
                        "[ERROR] GNURadio not installed—live mode unavailable."
                    )
                    self.finished.emit(False)
                    return
                self.progress.emit("[INFO] Live mode placeholder started.")
                self.finished.emit(True)
                return

            self.file = open(file_path, "rb")
            
            # Pre-allocate GPU memory if using batch processing
            if self.batch_size > 1 and self.device.type == 'cuda':
                self._setup_batch_buffers(nfft, out_size)
            
            # Emit initial throughput of 0
            self.throughputUpdated.emit(0.0)
            
            while True:
                if self._stop_flag:
                    self.progress.emit("[INFO] Stopping inference.")
                    if self.chunk_buffer:
                        # Process remaining chunks
                        self.process_batch(sample_rate, bandwidth, nfft, noverlap, dynamic_range, out_size)
                    self.file.close()
                    self.finished.emit(False)
                    return

                # Check pause state - wait while paused
                while self._pause_flag and not self._stop_flag:
                    time.sleep(0.1)  # Sleep briefly to avoid busy waiting
                
                # Check if we were stopped while paused
                if self._stop_flag:
                    continue

                raw = self.file.read(self.chunk_size * 8)
                if not raw:
                    if single_pass:
                        # Process remaining buffer
                        if self.chunk_buffer:
                            self.process_batch(sample_rate, bandwidth, nfft, noverlap, dynamic_range, out_size)
                        self.progress.emit("[INFO] Single-pass inference complete.")
                        break
                    else:
                        self.file.seek(0)
                        continue

                chunk_iq = np.frombuffer(raw, dtype=np.complex64)
                self.chunk_buffer.append(chunk_iq)
                
                # Process when batch is full or timeout
                if len(self.chunk_buffer) >= self.batch_size or \
                   (time.time() - self.last_batch_time) > self.batch_process_interval:
                    self.process_batch(sample_rate, bandwidth, nfft, noverlap, dynamic_range, out_size)
                    self.last_batch_time = time.time()

            self.file.close()
            self.finished.emit(True)

        except Exception:
            err = "".join(traceback.format_exception(*sys.exc_info()))
            self.progress.emit(f"[ERROR] {err}")
            self.finished.emit(False)

    def process_chunk(
        self, chunk_iq, samp_rate, bw, nfft, noverlap, dynamic_range, out_size
    ):
        import torch
        import torch.nn.functional as F
        
        # Initialize buffers and window on first run
        noverlap = min(noverlap, nfft - 1)
        hop = nfft - noverlap
        
        if not hasattr(self, '_initialized') or self._nfft != nfft or self._hop != hop:
            self._window = torch.hann_window(nfft, device=self.device)
            self._hop = hop
            self._nfft = nfft
            # Pre-allocate buffers to avoid repeated allocations
            self._chunk_buffer = torch.zeros(self.chunk_size, dtype=torch.complex64, device=self.device)
            self._stft_buffer = None  # Will be sized after first STFT
            self._initialized = True
        
        precision = self.params.get("precision", "fp32")
        
        # DEBUG: Detailed timing every 10 chunks to find bottleneck
        debug_timing = (self.chunk_count % 10 == 0)
        if debug_timing:
            torch.cuda.synchronize()
            t0 = time.time()
            times = {}
        
        # 1. Move IQ data to GPU (simplified direct transfer)
        if debug_timing:
            # Direct numpy to GPU transfer
            chunk_gpu = torch.from_numpy(chunk_iq).to(self.device, non_blocking=True)
            torch.cuda.synchronize()
            times['copy_to_gpu'] = (time.time() - t0) * 1000
            t0 = time.time()
        else:
            # Fast path without timing
            chunk_gpu = torch.from_numpy(chunk_iq).to(self.device, non_blocking=True)
        
        # 2. GPU STFT
        Zxx = torch.stft(
            chunk_gpu,
            n_fft=nfft,
            hop_length=self._hop,
            win_length=nfft,
            window=self._window,
            center=False,
            return_complex=True
        )
        
        if debug_timing:
            torch.cuda.synchronize()
            times['stft'] = (time.time() - t0) * 1000
            t0 = time.time()
        
        # 3. Combined operations to minimize kernel launches
        # FFT shift + power + log in fewer operations
        Zxx_shifted = torch.fft.fftshift(Zxx, dim=0)
        sxx_db = 10 * torch.log10(Zxx_shifted.abs().square() + 1e-12)
        
        if debug_timing:
            torch.cuda.synchronize()
            times['fft_power_log'] = (time.time() - t0) * 1000
            t0 = time.time()
        
        # 4. Normalization with in-place operations where possible
        vmax = sxx_db.max()
        vmin = vmax - dynamic_range
        sxx_norm = ((sxx_db - vmin) / (vmax - vmin + 1e-12)).clamp_(0, 1)
        
        if debug_timing:
            torch.cuda.synchronize()
            times['normalize'] = (time.time() - t0) * 1000
            t0 = time.time()
        
        # 5. Resize and prepare for model
        # Add batch and channel dims for interpolate
        sxx_norm = sxx_norm.unsqueeze(0).unsqueeze(0)
        resized = F.interpolate(
            sxx_norm, 
            size=(out_size, out_size),
            mode='bilinear',
            align_corners=False
        )
        
        # Flip vertically to match original np.flipud behavior
        # resized shape is [1, 1, H, W], we need to flip H dimension (dim=2)
        resized_flipped = torch.flip(resized, dims=[2])  # Flip vertical axis
        
        # Create 3 channels
        img_3ch = resized_flipped.expand(-1, 3, -1, -1)  # expand is more efficient than repeat
        
        if precision == "fp16":
            img_3ch = img_3ch.half()
        
        if debug_timing:
            torch.cuda.synchronize()
            times['resize_flip_channels'] = (time.time() - t0) * 1000
            t0 = time.time()
        
        # 6. Neural network inference - simplified without overhead
        with torch.inference_mode():
            out = self.model(img_3ch)[0]
        
        if debug_timing:
            torch.cuda.synchronize()
            times['model_inference'] = (time.time() - t0) * 1000
            t0 = time.time()
        
        # 7. Post-processing - move to CPU only what's needed
        boxes = out["boxes"].cpu().numpy()
        scores = out["scores"].cpu().numpy()
        keep = scores >= self.params["score_thresh"]
        boxes_k = boxes[keep]
        scores_k = scores[keep]
        
        # For display, get spectrogram on CPU (use the flipped version!)
        resized_cpu = resized_flipped[0, 0].cpu().numpy()
        
        if debug_timing:
            torch.cuda.synchronize()
            times['cpu_transfer'] = (time.time() - t0) * 1000
            
            # Silent collection for telemetry only
            total = sum(times.values())
        
        # Emit timing only occasionally to avoid overhead
        if self.chunk_count % 50 == 0:
            torch.cuda.synchronize()
            
            # Send real timing breakdowns to Grafana
            if debug_timing and times:
                # Push actual measured timings
                model_name = self.params.get("model_path", "unknown").split("/")[-1].replace(".pth", "")
                
                # First push all system metrics (GPU util, power, memory, CPU cores, etc)
                with inference_span(model_name, "pytorch", str(self.device), None, None):
                    pass  # This collects and pushes all system metrics
                
                # Then push detailed timing breakdown
                push_inference_timings(
                    (times.get('stft', 0) + times.get('fft_power_log', 0) + 
                     times.get('normalize', 0) + times.get('resize_flip_channels', 0)) / 1000,  # preprocessing
                    times.get('copy_to_gpu', 0) / 1000,  # tensor transfer
                    times.get('model_inference', 0) / 1000,  # forward pass
                    times.get('cpu_transfer', 0) / 1000,  # pixel2freq mapping (post-processing)
                    0.001,  # emit time (minimal)
                    model_name, "pytorch", str(self.device), None
                )
                
                # Update inference speed display
                total_time = sum(times.values())
                self.inferenceSpeedUpdated.emit(total_time)
        
        # Rest of emission code...
        bbs = [
            (float(x1), float(y1), float(x2), float(y2), float(sc))
            for (x1, y1, x2, y2), sc in zip(boxes_k, scores_k)
        ]

        time_span = app_settings["chunk_ms"] / 1000.0
        extent = [0.0, time_span, -bw / 2.0, bw / 2.0]
        self.spectrogramReady.emit(resized_cpu, bbs, tuple(extent))

        self.chunk_count += 1
        msg = {
            "chunk_id": self.chunk_count,
            "boxes": [
                {
                    "box_id": i,
                    "x1": round(x1, 2),
                    "y1": round(y1, 2),
                    "x2": round(x2, 2),
                    "y2": round(y2, 2),
                    "score": round(sc, 3),
                }
                for i, (x1, y1, x2, y2, sc) in enumerate(bbs)
            ],
            "elapsed_seconds": round(
                self.chunk_count * (app_settings["chunk_ms"] / 1000.0), 2
            ),
        }
        self.detectionJson.emit(msg)

        if self.params.get("single_pass", False) and bbs:
            if self.single_pass_dir is None:
                ts = datetime.now().strftime("%Y%m%d_%H%M%S")
                base = os.path.splitext(os.path.basename(self.params["file_path"]))[0]
                self.single_pass_dir = os.path.join(
                    DATA_DIR, "spec_image", f"{base}_inf_{ts}"
                )
                os.makedirs(self.single_pass_dir, exist_ok=True)
                self.progress.emit(
                    f"[INFO] Created single-pass dir: {self.single_pass_dir}"
                )

            from matplotlib import pyplot as plt

            img_file = os.path.join(
                self.single_pass_dir, f"infer_{self.chunk_count:03d}.png"
            )
            plt.imsave(img_file, resized_cpu, cmap="gray", origin="upper")

            bjson = os.path.join(
                self.single_pass_dir, f"infer_{self.chunk_count:03d}_bboxes.json"
            )
            with open(bjson, "w") as jf:
                json.dump(
                    {
                        "image": os.path.basename(img_file),
                        "width": out_size,
                        "height": out_size,
                        "bboxes": [
                            {
                                "x_min": float(x1),
                                "y_min": float(y1),
                                "x_max": float(x2),
                                "y_max": float(y2),
                                "score": round(sc, 3),
                            }
                            for x1, y1, x2, y2, sc in bbs
                        ],
                    },
                    jf,
                    indent=2,
                )
            self.progress.emit(
                f"[INFO] Saved inference chunk {self.chunk_count:03d}"
            )
    
    def _setup_batch_buffers(self, nfft, out_size):
        """Pre-allocate GPU buffers for batch processing"""
        self.progress.emit(f"[INFO] Setting up batch buffers for batch_size={self.batch_size}")
        
        # Pre-allocate GPU chunk buffer
        self.gpu_chunk_buffer = torch.zeros(
            (self.batch_size, self.chunk_size), 
            dtype=torch.complex64, 
            device=self.device
        )
        
        # Pre-allocate window for STFT
        self._window = torch.hann_window(nfft, device=self.device)
        self._nfft = nfft
        self._hop = nfft - min(app_settings["noverlap"], nfft - 1)
        
        # Pre-allocate output buffer
        self.gpu_spect_buffer = torch.zeros(
            (self.batch_size, 3, out_size, out_size),
            dtype=torch.float32,
            device=self.device
        )
    
    def process_batch(self, sample_rate, bandwidth, nfft, noverlap, dynamic_range, out_size):
        """Process a batch of chunks together"""
        if not self.chunk_buffer:
            return
        
        # Check pause state - wait while paused
        while self._pause_flag and not self._stop_flag:
            time.sleep(0.1)  # Sleep briefly to avoid busy waiting
        
        # Check if we were stopped while paused
        if self._stop_flag:
            return
        
        import torch
        import torch.nn.functional as F
        
        batch_size = len(self.chunk_buffer)
        precision = self.params.get("precision", "fp32")
        
        # Initialize on first batch
        if not hasattr(self, '_window'):
            self._window = torch.hann_window(nfft, device=self.device)
            self._hop = nfft - min(noverlap, nfft - 1)
            self._nfft = nfft
        
        # Stack chunks into batch
        if self.device.type == 'cuda' and hasattr(self, 'gpu_chunk_buffer'):
            # Use pre-allocated buffer
            for i, chunk in enumerate(self.chunk_buffer[:batch_size]):
                self.gpu_chunk_buffer[i, :len(chunk)].copy_(torch.from_numpy(chunk))
            batch_gpu = self.gpu_chunk_buffer[:batch_size]
        else:
            # Direct stacking
            batch_gpu = torch.stack([
                torch.from_numpy(chunk).to(self.device, non_blocking=True) 
                for chunk in self.chunk_buffer[:batch_size]
            ])
        
        # Process batch through preprocessing
        batch_spectrograms = []
        
        for i in range(batch_size):
            chunk_gpu = batch_gpu[i]
            
            # STFT
            Zxx = torch.stft(
                chunk_gpu,
                n_fft=nfft,
                hop_length=self._hop,
                win_length=nfft,
                window=self._window,
                center=False,
                return_complex=True
            )
            
            # FFT shift + power + log
            Zxx_shifted = torch.fft.fftshift(Zxx, dim=0)
            sxx_db = 10 * torch.log10(Zxx_shifted.abs().square() + 1e-12)
            
            # Normalization
            vmax = sxx_db.max()
            vmin = vmax - dynamic_range
            sxx_norm = ((sxx_db - vmin) / (vmax - vmin + 1e-12)).clamp_(0, 1)
            
            # Resize
            sxx_norm = sxx_norm.unsqueeze(0).unsqueeze(0)
            resized = F.interpolate(
                sxx_norm, 
                size=(out_size, out_size),
                mode='bilinear',
                align_corners=False
            )
            
            # Flip vertically
            resized_flipped = torch.flip(resized, dims=[2])
            
            # Create 3 channels
            img_3ch = resized_flipped.expand(-1, 3, -1, -1).squeeze(0)
            
            batch_spectrograms.append(img_3ch)
        
        # Stack into batch tensor
        batch_tensor = torch.stack(batch_spectrograms)
        
        if precision == "fp16":
            batch_tensor = batch_tensor.half()
        
        # Batch inference - ONLY TIME THIS PART
        torch.cuda.synchronize() if self.device.type == 'cuda' else None
        inf_start = time.time()
        
        with torch.inference_mode():
            outputs = self.model(batch_tensor)
        
        torch.cuda.synchronize() if self.device.type == 'cuda' else None
        inf_time = (time.time() - inf_start) * 1000 / batch_size  # ms per chunk
        
        # Process and emit individual results
        time_span = app_settings["chunk_ms"] / 1000.0
        extent = [0.0, time_span, -bandwidth / 2.0, bandwidth / 2.0]
        
        for i, output in enumerate(outputs):
            # Get results
            boxes = output["boxes"].cpu().numpy()
            scores = output["scores"].cpu().numpy()
            keep = scores >= self.params["score_thresh"]
            boxes_k = boxes[keep]
            scores_k = scores[keep]
            
            # Get spectrogram for display
            resized_cpu = batch_spectrograms[i][0].cpu().numpy()
            
            # Format bounding boxes
            bbs = [
                (float(x1), float(y1), float(x2), float(y2), float(sc))
                for (x1, y1, x2, y2), sc in zip(boxes_k, scores_k)
            ]
            
            # Emit results
            self.spectrogramReady.emit(resized_cpu, bbs, tuple(extent))
            
            self.chunk_count += 1
            msg = {
                "chunk_id": self.chunk_count,
                "boxes": [
                    {
                        "box_id": j,
                        "x1": round(x1, 2),
                        "y1": round(y1, 2),
                        "x2": round(x2, 2),
                        "y2": round(y2, 2),
                        "score": round(sc, 3),
                    }
                    for j, (x1, y1, x2, y2, sc) in enumerate(bbs)
                ],
                "elapsed_seconds": round(
                    self.chunk_count * (app_settings["chunk_ms"] / 1000.0), 2
                ),
            }
            self.detectionJson.emit(msg)
            
            # Save single-pass results if needed
            if self.params.get("single_pass", False) and bbs:
                if self.single_pass_dir is None:
                    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
                    base = os.path.splitext(os.path.basename(self.params["file_path"]))[0]
                    self.single_pass_dir = os.path.join(
                        DATA_DIR, "spec_image", f"{base}_inf_{ts}"
                    )
                    os.makedirs(self.single_pass_dir, exist_ok=True)
                    self.progress.emit(
                        f"[INFO] Created single-pass dir: {self.single_pass_dir}"
                    )
                
                from matplotlib import pyplot as plt
                
                img_file = os.path.join(
                    self.single_pass_dir, f"infer_{self.chunk_count:03d}.png"
                )
                plt.imsave(img_file, resized_cpu, cmap="gray", origin="upper")
                
                bjson = os.path.join(
                    self.single_pass_dir, f"infer_{self.chunk_count:03d}_bboxes.json"
                )
                with open(bjson, "w") as jf:
                    json.dump(
                        {
                            "image": os.path.basename(img_file),
                            "width": out_size,
                            "height": out_size,
                            "bboxes": [
                                {
                                    "x_min": float(x1),
                                    "y_min": float(y1),
                                    "x_max": float(x2),
                                    "y_max": float(y2),
                                    "score": round(sc, 3),
                                }
                                for x1, y1, x2, y2, sc in bbs
                            ],
                        },
                        jf,
                        indent=2,
                    )
                self.progress.emit(
                    f"[INFO] Saved inference chunk {self.chunk_count:03d}"
                )
        
        # Clear processed chunks
        self.chunk_buffer = self.chunk_buffer[batch_size:]
        
        # Emit the ACTUAL inference timing
        self.inferenceSpeedUpdated.emit(inf_time)
        
        # Calculate and emit throughput
        current_time = time.time()
        elapsed_time = current_time - self.start_time
        if elapsed_time > 0:
            throughput = self.chunk_count / elapsed_time
            # Always emit on first batch or after interval
            if self.chunk_count <= batch_size or \
               current_time - self.last_throughput_update >= self.throughput_update_interval:
                self.throughputUpdated.emit(throughput)
                self.last_throughput_update = current_time
        
        if self.chunk_count % 10 == 0:
            self.progress.emit(f"[INFO] Batch of {batch_size} chunks processed (~{inf_time:.1f} ms inference per chunk)")


================================================================================
# FILE: ARCADE/src/tensorcade/widgets.py
================================================================================
# widgets.py
"""
Module for PyQt5 UI widgets and dialogs.
"""

import os
import json
import numpy as np
from collections import deque
from PyQt5.QtWidgets import (
    QApplication, QMainWindow, QWidget, QVBoxLayout, QHBoxLayout,
    QPushButton, QLabel, QLineEdit, QCheckBox, QSpinBox, QFileDialog,
    QDoubleSpinBox, QComboBox, QProgressBar, QFormLayout, QDialog,
    QDialogButtonBox, QGroupBox, QPlainTextEdit, QTabWidget, QSizePolicy,
    QGridLayout, QAction
)
from PyQt5.QtCore import Qt, QTimer, QThread, pyqtSignal
from PyQt5.QtGui import QTextCursor
import pyqtgraph as pg

from .utils import make_help_label, ScientificDoubleSpinBox
from .workers import ChunkWorker, TrainWorker, InferenceWorker
from .config import app_settings, DATA_DIR, MODELS_DIR


class MasterConfigDialog(QDialog):
    """
    Dialog for modifying global configuration settings.
    """
    def __init__(self, parent=None):
        super().__init__(parent)
        self.setWindowTitle("Configuration Settings")
        self.setMinimumSize(600, 240)
        main_layout = QVBoxLayout()
        main_layout.setSpacing(5)
        main_layout.setContentsMargins(5, 5, 5, 5)
        grid = QGridLayout()
        grid.setSpacing(5)
        row = 0
        col = 0
        # Chunk (ms)
        chunk_label = QLabel("Chunk (ms):")
        self.chunkMsSpin = QDoubleSpinBox()
        self.chunkMsSpin.setRange(1, 100000)
        self.chunkMsSpin.setValue(app_settings["chunk_ms"])
        chunk_help = make_help_label("In data processing, a 'chunk' is a portion processed at once.\nControls how data is segmented.")
        grid.addWidget(chunk_label, row, col)
        col += 1
        grid.addWidget(self.chunkMsSpin, row, col)
        col += 1
        grid.addWidget(chunk_help, row, col)
        col += 1
        # NFFT
        nfft_label = QLabel("NFFT:")
        self.nfftSpin = QSpinBox()
        self.nfftSpin.setRange(1, 999999)
        self.nfftSpin.setValue(app_settings["nfft"])
        nfft_help = make_help_label("Number of FFT points. Determines frequency resolution.")
        grid.addWidget(nfft_label, row, col)
        col += 1
        grid.addWidget(self.nfftSpin, row, col)
        col += 1
        grid.addWidget(nfft_help, row, col)
        row += 1
        col = 0
        # NOverlap
        nover_label = QLabel("NOverlap:")
        self.noverSpin = QSpinBox()
        self.noverSpin.setRange(0, 999999)
        self.noverSpin.setValue(app_settings["noverlap"])
        nover_help = make_help_label("Number of overlapping samples between successive FFT segments.")
        grid.addWidget(nover_label, row, col)
        col += 1
        grid.addWidget(self.noverSpin, row, col)
        col += 1
        grid.addWidget(nover_help, row, col)
        col += 1
        # Dynamic Range
        dynamic_label = QLabel("Dyn Range (dB):")
        self.dynamicSpin = QDoubleSpinBox()
        self.dynamicSpin.setRange(0, 200)
        self.dynamicSpin.setValue(app_settings["dynamic_range"])
        dynamic_help = make_help_label("Range between smallest and largest values,\ncontrolling spectrogram scaling.")
        grid.addWidget(dynamic_label, row, col)
        col += 1
        grid.addWidget(self.dynamicSpin, row, col)
        col += 1
        grid.addWidget(dynamic_help, row, col)
        row += 1
        col = 0
        # Output Size
        out_size_label = QLabel("Output Size:")
        self.outSizeSpin = QSpinBox()
        self.outSizeSpin.setRange(1, 4096)
        self.outSizeSpin.setValue(app_settings["out_size"])
        out_size_help = make_help_label("Dimensions of the final image/feature map used in training/inference.")
        grid.addWidget(out_size_label, row, col)
        col += 1
        grid.addWidget(self.outSizeSpin, row, col)
        col += 1
        grid.addWidget(out_size_help, row, col)
        col += 1
        # Backbone
        backbone_label = QLabel("Backbone:")
        self.backboneCombo = QComboBox()
        self.backboneCombo.addItems(["resnet18", "resnet50"])
        self.backboneCombo.setCurrentText(app_settings["backbone"])
        backbone_help = make_help_label("Primary CNN (e.g., ResNet) for feature extraction in detection.")
        grid.addWidget(backbone_label, row, col)
        col += 1
        grid.addWidget(self.backboneCombo, row, col)
        col += 1
        grid.addWidget(backbone_help, row, col)
        row += 1
        col = 0
        # Pretrained
        pretrained_label = QLabel("Pretrained:")
        self.pretrainedCheck = QCheckBox()
        self.pretrainedCheck.setChecked(app_settings["pretrained"])
        pretrained_help = make_help_label("If checked, use a backbone pretrained on a large dataset (e.g., ImageNet).")
        grid.addWidget(pretrained_label, row, col)
        col += 1
        grid.addWidget(self.pretrainedCheck, row, col)
        col += 1
        grid.addWidget(pretrained_help, row, col)
        col += 1
        # Trainable Layers
        tl_label = QLabel("Trainable Layers:")
        self.tlSpin = QSpinBox()
        self.tlSpin.setRange(0, 5)
        self.tlSpin.setValue(app_settings["trainable_layers"])
        tl_help = make_help_label("Number of backbone layers that remain trainable (unfrozen).")
        grid.addWidget(tl_label, row, col)
        col += 1
        grid.addWidget(self.tlSpin, row, col)
        col += 1
        grid.addWidget(tl_help, row, col)
        row += 1
        col = 0
        # Use GPU
        gpu_label = QLabel("Use GPU:")
        self.gpuCheck = QCheckBox()
        self.gpuCheck.setChecked(app_settings["use_gpu"])
        gpu_help = make_help_label("If checked, inference/training will try to use CUDA if available.")
        grid.addWidget(gpu_label, row, col)
        col += 1
        grid.addWidget(self.gpuCheck, row, col)
        col += 1
        grid.addWidget(gpu_help, row, col)
        row += 1
        col = 0
        # NEW: Early Stop Patience
        early_stop_label = QLabel("Early Stop Patience:")
        self.earlyStopSpin = QSpinBox()
        self.earlyStopSpin.setRange(1, 100)
        self.earlyStopSpin.setValue(app_settings.get("early_stop_patience", 10))
        early_stop_help = make_help_label("Number of epochs with no improvement before training stops early.")
        grid.addWidget(early_stop_label, row, col)
        col += 1
        grid.addWidget(self.earlyStopSpin, row, col)
        col += 1
        grid.addWidget(early_stop_help, row, col)
        row += 1
        col = 0

        main_layout.addLayout(grid)
        button_box = QDialogButtonBox(QDialogButtonBox.Save | QDialogButtonBox.Close)
        button_box.accepted.connect(self.saveSettings)
        button_box.rejected.connect(self.close)
        main_layout.addWidget(button_box)
        self.setLayout(main_layout)

    def saveSettings(self):
        app_settings["chunk_ms"] = self.chunkMsSpin.value()
        app_settings["nfft"] = self.nfftSpin.value()
        app_settings["noverlap"] = self.noverSpin.value()
        app_settings["dynamic_range"] = self.dynamicSpin.value()
        app_settings["out_size"] = self.outSizeSpin.value()
        app_settings["backbone"] = self.backboneCombo.currentText()
        app_settings["pretrained"] = self.pretrainedCheck.isChecked()
        app_settings["trainable_layers"] = self.tlSpin.value()
        app_settings["use_gpu"] = self.gpuCheck.isChecked()
        app_settings["early_stop_patience"] = self.earlyStopSpin.value()
        from .logging_config import logger
        logger.info("Global settings updated via MasterConfigDialog.")
        self.close()


class ResourceUsagePanel(QWidget):
    """
    Panel to display system resource usage.
    """
    def __init__(self, parent=None):
        super().__init__(parent)
        layout = QVBoxLayout(self)
        self.cpuLabel = QLabel("CPU: ?")
        self.cpuLabel.setStyleSheet("color: red;")
        self.ramLabel = QLabel("RAM: ?")
        self.ramLabel.setStyleSheet("color: green;")
        self.gpuLabel = QLabel("GPU: ?")
        self.gpuLabel.setStyleSheet("color: blue;")
        self.infLabel = QLabel("Inference Speed: ? ms")
        self.infLabel.setStyleSheet("color: orange;")
        self.throughputLabel = QLabel("Total Throughput: ? chunks/sec")
        self.throughputLabel.setStyleSheet("color: purple;")
        self.totalTimeLabel = QLabel("Total Time/Chunk: ? ms")
        self.totalTimeLabel.setStyleSheet("color: cyan;")
        layout.addWidget(self.cpuLabel)
        layout.addWidget(self.ramLabel)
        layout.addWidget(self.gpuLabel)
        layout.addWidget(self.infLabel)
        layout.addWidget(self.throughputLabel)
        layout.addWidget(self.totalTimeLabel)
        layout.addStretch(1)
        self.setLayout(layout)
        self.timer = QTimer(self)
        self.timer.timeout.connect(self.updateUsage)
        self.timer.start(1000)

    def setInferenceSpeed(self, ms):
        self.infLabel.setText(f"Inference Speed: {ms:.1f} ms")
    
    def setThroughput(self, chunks_per_sec):
        self.throughputLabel.setText(f"Total Throughput: {chunks_per_sec:.1f} chunks/sec")
        # Calculate and display total time per chunk (inverse of throughput)
        if chunks_per_sec > 0:
            total_time_ms = 1000.0 / chunks_per_sec
            self.totalTimeLabel.setText(f"Total Time/Chunk: {total_time_ms:.1f} ms")
        else:
            self.totalTimeLabel.setText("Total Time/Chunk: ? ms")

    def updateUsage(self):
        cpu_percent = 0.0
        ram_percent = 0.0
        ram_used_gb = 0.0
        ram_total_gb = 1.0
        gpu_percent = 0.0
        gpu_used_gb = 0.0
        gpu_total_gb = 1.0

        try:
            import psutil
            cpu_percent = psutil.cpu_percent()
            mem = psutil.virtual_memory()
            ram_percent = mem.percent
            ram_used_gb = mem.used / (1024 ** 3)
            ram_total_gb = mem.total / (1024 ** 3)
        except ImportError:
            pass

        try:
            import nvidia_smi
            nvidia_smi.nvmlInit()
            handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)
            util = nvidia_smi.nvmlDeviceGetUtilizationRates(handle)
            gpu_percent = float(util.gpu)
            mem_info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)
            gpu_used_gb = mem_info.used / (1024 ** 3)
            gpu_total_gb = mem_info.total / (1024 ** 3)
            nvidia_smi.nvmlShutdown()
        except Exception:
            pass

        self.cpuLabel.setText(f"CPU: {cpu_percent:.1f}%")
        self.ramLabel.setText(f"RAM: {ram_used_gb:.1f}/{ram_total_gb:.1f} GiB ({ram_percent:.1f}%)")
        self.gpuLabel.setText(f"GPU: {gpu_used_gb:.1f}/{gpu_total_gb:.1f} GiB ({gpu_percent:.1f}%)")


class CreateImagesBox(QGroupBox):
    """
    Group box for creating spectrogram images from IQ data.
    """
    logSignal = pyqtSignal(str)
    def __init__(self, parent=None):
        super().__init__("Create Images", parent)
        layout = QFormLayout(self)
        layout.setContentsMargins(5, 5, 5, 5)
        default_iq_dir = os.path.join(DATA_DIR, "iq")
        self.inputDirEdit = QLineEdit(default_iq_dir)
        self.inputDirEdit.setMinimumWidth(250)
        self.inputDirEdit.setToolTip(self.inputDirEdit.text())
        self.inputDirEdit.textChanged.connect(lambda txt: self.inputDirEdit.setToolTip(txt))
        btn_browse = QPushButton("Browse...")
        btn_browse.clicked.connect(self.browseDir)
        hb = QHBoxLayout()
        hb.setSpacing(5)
        hb.addWidget(self.inputDirEdit)
        hb.addWidget(btn_browse)
        container = QWidget()
        container.setLayout(hb)
        layout.addRow("Input Dir:", container)
        self.progressBar = QProgressBar()
        layout.addRow("Progress:", self.progressBar)
        self.startBtn = QPushButton("Start")
        self.stopBtn = QPushButton("Stop")
        self.stopBtn.setEnabled(False)
        hb2 = QHBoxLayout()
        hb2.setSpacing(5)
        hb2.addWidget(self.startBtn)
        hb2.addWidget(self.stopBtn)
        layout.addRow(hb2)
        self.setLayout(layout)
        self.thread = None
        self.worker = None
        self.startBtn.clicked.connect(self.startTask)
        self.stopBtn.clicked.connect(self.stopTask)

    def browseDir(self):
        d = QFileDialog.getExistingDirectory(self, "Select Input Dir", self.inputDirEdit.text())
        if d:
            self.inputDirEdit.setText(d)

    def startTask(self):
        self.progressBar.setValue(0)
        self.startBtn.setEnabled(False)
        self.stopBtn.setEnabled(True)
        inp_dir = self.inputDirEdit.text().strip()
        self.worker = ChunkWorker(inp_dir)
        self.thread = QThread()
        self.worker.moveToThread(self.thread)
        self.worker.progress.connect(self.logSignal.emit)
        self.worker.progressPercent.connect(self.progressBar.setValue)
        self.worker.finished.connect(self.onFinished)
        self.thread.started.connect(self.worker.run)
        self.thread.start()

    def stopTask(self):
        if self.worker:
            self.worker.stop()

    def onFinished(self, success):
        self.startBtn.setEnabled(True)
        self.stopBtn.setEnabled(False)
        if self.thread:
            self.thread.quit()
            self.thread.wait()


class TrainModelBox(QGroupBox):
    """
    Group box for training the detection model.
    """
    logSignal = pyqtSignal(str)
    def __init__(self, parent=None):
        super().__init__("Train Model", parent)
        layout = QVBoxLayout(self)
        layout.setSpacing(5)
        layout.setContentsMargins(0, 0, 0, 0)
        dataRow = QHBoxLayout()
        dataRow.setSpacing(5)
        dataLabel = QLabel("Data Dir:")
        default_spec_dir = os.path.join(DATA_DIR, "spec_image")
        self.dataDirEdit = QLineEdit(default_spec_dir)
        self.dataDirEdit.setMinimumWidth(250)
        self.dataDirEdit.setToolTip(self.dataDirEdit.text())
        self.dataDirEdit.textChanged.connect(lambda txt: self.dataDirEdit.setToolTip(txt))
        btn_data = QPushButton("Browse...")
        btn_data.clicked.connect(self.browseData)
        dataRow.addWidget(dataLabel)
        dataRow.addWidget(self.dataDirEdit)
        dataRow.addWidget(btn_data)
        layout.addLayout(dataRow)
        row1 = QHBoxLayout()
        row1.setSpacing(5)
        epochLabel = QLabel("Epochs:")
        self.epochSpin = QSpinBox()
        self.epochSpin.setRange(1, 99999)
        self.epochSpin.setValue(5)
        epochHelp = make_help_label("Number of complete passes through the training dataset.")
        batchLabel = QLabel("Batch Size:")
        self.batchSpin = QSpinBox()
        self.batchSpin.setRange(1, 1024)
        self.batchSpin.setValue(2)
        batchHelp = make_help_label("Number of samples processed before weights are updated.")
        row1.addWidget(epochLabel)
        row1.addWidget(self.epochSpin)
        row1.addWidget(epochHelp)
        row1.addSpacing(20)
        row1.addWidget(batchLabel)
        row1.addWidget(self.batchSpin)
        row1.addWidget(batchHelp)
        layout.addLayout(row1)
        row2 = QHBoxLayout()
        row2.setSpacing(5)
        lrLabel = QLabel("Learning Rate:")
        self.lrSpin = QDoubleSpinBox()
        self.lrSpin.setRange(1e-6, 1.0)
        self.lrSpin.setDecimals(6)
        self.lrSpin.setValue(0.0001)
        lrHelp = make_help_label("Controls how quickly the model learns. Too high overshoots, too low is slow.")
        kfLabel = QLabel("K-Folds:")
        self.kfSpin = QSpinBox()
        self.kfSpin.setRange(1, 20)
        self.kfSpin.setValue(1)
        kfHelp = make_help_label("Number of cross-validation folds. Set to 1 for no CV.")
        row2.addWidget(lrLabel)
        row2.addWidget(self.lrSpin)
        row2.addWidget(lrHelp)
        row2.addSpacing(20)
        row2.addWidget(kfLabel)
        row2.addWidget(self.kfSpin)
        row2.addWidget(kfHelp)
        layout.addLayout(row2)
        outRow = QHBoxLayout()
        outRow.setSpacing(5)
        outLabel = QLabel("Output Model:")
        default_model_path = os.path.join(MODELS_DIR, "detector.pth")
        self.outModelEdit = QLineEdit(default_model_path)
        self.outModelEdit.setMinimumWidth(250)
        self.outModelEdit.setToolTip(self.outModelEdit.text())
        self.outModelEdit.textChanged.connect(lambda txt: self.outModelEdit.setToolTip(txt))
        outRow.addWidget(outLabel)
        outRow.addWidget(self.outModelEdit)
        layout.addLayout(outRow)
        progRow = QHBoxLayout()
        progRow.setSpacing(5)
        progLabel = QLabel("Progress:")
        self.progressBar = QProgressBar()
        progRow.addWidget(progLabel)
        progRow.addWidget(self.progressBar)
        layout.addLayout(progRow)
        btnRow = QHBoxLayout()
        btnRow.setSpacing(5)
        self.startBtn = QPushButton("Start")
        self.stopBtn = QPushButton("Stop")
        self.stopBtn.setEnabled(False)
        btnRow.addWidget(self.startBtn)
        btnRow.addWidget(self.stopBtn)
        layout.addLayout(btnRow)
        self.setLayout(layout)
        self.thread = None
        self.worker = None
        self.startBtn.clicked.connect(self.startTask)
        self.stopBtn.clicked.connect(self.stopTask)

    def browseData(self):
        d = QFileDialog.getExistingDirectory(self, "Select Data Dir", self.dataDirEdit.text())
        if d:
            self.dataDirEdit.setText(d)

    def startTask(self):
        self.progressBar.setValue(0)
        self.startBtn.setEnabled(False)
        self.stopBtn.setEnabled(True)
        dd = self.dataDirEdit.text().strip()
        ep = self.epochSpin.value()
        bs = self.batchSpin.value()
        lr = self.lrSpin.value()
        kf = self.kfSpin.value()
        om = self.outModelEdit.text().strip()
        params = {
            "epochs": ep,
            "batch_size": bs,
            "lr": lr,
            "k_folds": kf,
            "out_model": om
        }
        self.worker = TrainWorker(dd, params)
        self.thread = QThread()
        self.worker.moveToThread(self.thread)
        self.worker.progress.connect(self.logSignal.emit)
        self.worker.progressPercent.connect(self.progressBar.setValue)
        self.worker.finished.connect(self.onFinished)
        self.thread.started.connect(self.worker.run)
        self.thread.start()

    def stopTask(self):
        if self.worker:
            self.worker.stop()

    def onFinished(self, success):
        self.startBtn.setEnabled(True)
        self.stopBtn.setEnabled(False)
        if self.thread:
            self.thread.quit()
            self.thread.wait()


class FilePropertiesDialog(QDialog):
    """
    Dialog for setting file properties for IQ data.
    """
    def __init__(self, parent=None):
        super().__init__(parent)
        self.setWindowTitle("File Properties")
        self.setMinimumWidth(350)
        self.sampleRate = 40e6
        self.bandwidth = 20e6
        self.signalFormat = "Complex (I/Q) - 32-bit float"
        layout = QFormLayout()
        layout.setContentsMargins(5, 5, 5, 5)
        srRow = QHBoxLayout()
        srRow.setSpacing(5)
        self.srSpin = ScientificDoubleSpinBox()
        self.srSpin.setRange(1e3, 2e9)
        self.srSpin.setValue(self.sampleRate)
        srHelp = make_help_label("Sample Rate (Hz). e.g., 1e6 = 1 MHz.")
        srRow.addWidget(self.srSpin)
        srRow.addWidget(srHelp)
        srContainer = QWidget()
        srContainer.setLayout(srRow)
        layout.addRow("Sample Rate (Hz):", srContainer)
        bwRow = QHBoxLayout()
        bwRow.setSpacing(5)
        self.bwSpin = ScientificDoubleSpinBox()
        self.bwSpin.setRange(1e3, 2e9)
        self.bwSpin.setValue(self.bandwidth)
        bwHelp = make_help_label("Bandwidth (Hz). e.g., 20e6 = 20 MHz.")
        bwRow.addWidget(self.bwSpin)
        bwRow.addWidget(bwHelp)
        bwContainer = QWidget()
        bwContainer.setLayout(bwRow)
        layout.addRow("Bandwidth (Hz):", bwContainer)
        self.sigFormatCombo = QComboBox()
        self.sigFormatCombo.addItems([
            "Complex (I/Q) - 32-bit float",
            "Complex (I/Q) - 16-bit int",
            "Complex (I/Q) - 8-bit int"
        ])
        layout.addRow("Signal Format:", self.sigFormatCombo)
        buttonBox = QDialogButtonBox(QDialogButtonBox.Ok | QDialogButtonBox.Cancel)
        buttonBox.accepted.connect(self.accept)
        buttonBox.rejected.connect(self.reject)
        layout.addWidget(buttonBox)
        self.setLayout(layout)

    def accept(self):
        self.sampleRate = self.srSpin.value()
        self.bandwidth = self.bwSpin.value()
        self.signalFormat = self.sigFormatCombo.currentText()
        super().accept()


class OfflineWidget(QWidget):
    """
    Widget for offline IQ data inference.
    """
    def __init__(self, parent=None):
        super().__init__(parent)
        layout = QGridLayout(self)
        layout.setSpacing(0)
        layout.setContentsMargins(0, 0, 0, 0)
        layout.addWidget(QLabel("IQ File:"), 0, 0)
        # Pre-load IQ file path for speed
        default_iq_file = os.path.join(DATA_DIR, "iq", "825mhz_gap", "825MHz_wide.sigmf-data")
        self.filePathEdit = QLineEdit(default_iq_file)
        self.filePathEdit.setMinimumWidth(250)
        self.filePathEdit.setToolTip(self.filePathEdit.text())
        self.filePathEdit.textChanged.connect(lambda t: self.filePathEdit.setToolTip(t))
        browse_file = QPushButton("Browse IQ File")
        hb1 = QHBoxLayout()
        hb1.setSpacing(0)
        hb1.setContentsMargins(0, 0, 0, 0)
        hb1.addWidget(self.filePathEdit)
        hb1.addWidget(browse_file)
        fileContainer = QWidget()
        fileContainer.setLayout(hb1)
        layout.addWidget(fileContainer, 0, 1, 1, 3)
        browse_file.clicked.connect(self.browseFile)
        layout.addWidget(QLabel("Score Thresh:"), 1, 0)
        stRow = QHBoxLayout()
        stRow.setSpacing(0)
        stRow.setContentsMargins(0, 0, 0, 0)
        self.sthrSpin = QDoubleSpinBox()
        self.sthrSpin.setRange(0, 1)
        self.sthrSpin.setValue(0.9)
        stHelp = make_help_label("Minimum confidence score to keep a detection (0 to 1).")
        stRow.addWidget(self.sthrSpin)
        stRow.addWidget(stHelp)
        stContainer = QWidget()
        stContainer.setLayout(stRow)
        layout.addWidget(stContainer, 1, 1)
        # Set default properties for pre-loaded file
        if "825MHz" in default_iq_file:
            self.offlineSampleRate = 20e6  # 20 MHz sample rate
            self.offlineBandwidth = 20e6   # 20 MHz bandwidth
        else:
            self.offlineSampleRate = 20e6
            self.offlineBandwidth = 20e6
        self.offlineSignalFormat = "Complex (I/Q) - 32-bit float"
        self.setLayout(layout)

    def browseFile(self):
        fn, _ = QFileDialog.getOpenFileName(
            self,
            "Select IQ file",
            os.getcwd(),
            "IQ Files (*.sigmf-data *.bin *.dat *.raw *.tmp)"
        )
        if fn:
            self.filePathEdit.setText(fn)
            self.openFilePropsDialog()

    def openFilePropsDialog(self):
        dlg = FilePropertiesDialog(self)
        if dlg.exec_() == QDialog.Accepted:
            self.offlineSampleRate = dlg.sampleRate
            self.offlineBandwidth = dlg.bandwidth
            self.offlineSignalFormat = dlg.signalFormat


class LiveWidget(QWidget):
    """
    Widget for live IQ data inference.
    """
    def __init__(self, parent=None):
        super().__init__(parent)
        layout = QGridLayout(self)
        layout.setSpacing(0)
        layout.setContentsMargins(0, 0, 0, 0)
        srLabel = QLabel("Sample Rate (Hz):")
        srHelp = make_help_label("Sample Rate for bladeRF or other SDR (Hz). e.g., 1e6.")
        self.srSpin = ScientificDoubleSpinBox()
        self.srSpin.setRange(1e3, 1e9)
        self.srSpin.setValue(1e6)
        srRow = QHBoxLayout()
        srRow.setSpacing(0)
        srRow.setContentsMargins(0, 0, 0, 0)
        srRow.addWidget(self.srSpin)
        srRow.addWidget(srHelp)
        srContainer = QWidget()
        srContainer.setLayout(srRow)
        layout.addWidget(srLabel, 0, 0)
        layout.addWidget(srContainer, 0, 1)
        freqLabel = QLabel("Frequency (Hz):")
        freqHelp = make_help_label("Tuner center frequency for bladeRF (Hz). e.g., 446e6.")
        self.freqSpin = ScientificDoubleSpinBox()
        self.freqSpin.setRange(1e3, 1e12)
        self.freqSpin.setValue(446e6)
        freqRow = QHBoxLayout()
        freqRow.setSpacing(0)
        freqRow.setContentsMargins(0, 0, 0, 0)
        freqRow.addWidget(self.freqSpin)
        freqRow.addWidget(freqHelp)
        freqContainer = QWidget()
        freqContainer.setLayout(freqRow)
        layout.addWidget(freqLabel, 0, 2)
        layout.addWidget(freqContainer, 0, 3)
        gainLabel = QLabel("Gain:")
        gainHelp = make_help_label("RF gain in dB. e.g., 40.0.")
        self.gainSpin = QDoubleSpinBox()
        self.gainSpin.setRange(0, 100)
        self.gainSpin.setValue(40.0)
        gainRow = QHBoxLayout()
        gainRow.setSpacing(0)
        gainRow.setContentsMargins(0, 0, 0, 0)
        gainRow.addWidget(self.gainSpin)
        gainRow.addWidget(gainHelp)
        gainContainer = QWidget()
        gainContainer.setLayout(gainRow)
        layout.addWidget(gainLabel, 1, 0)
        layout.addWidget(gainContainer, 1, 1)
        bwLabel = QLabel("Bandwidth (Hz):")
        bwHelp = make_help_label("Filter bandwidth for bladeRF (Hz). e.g., 20e6.")
        self.bwSpin = ScientificDoubleSpinBox()
        self.bwSpin.setRange(1e3, 1e9)
        self.bwSpin.setValue(20e6)
        bwRow = QHBoxLayout()
        bwRow.setSpacing(0)
        bwRow.setContentsMargins(0, 0, 0, 0)
        bwRow.addWidget(self.bwSpin)
        bwRow.addWidget(bwHelp)
        bwContainer = QWidget()
        bwContainer.setLayout(bwRow)
        layout.addWidget(bwLabel, 1, 2)
        layout.addWidget(bwContainer, 1, 3)
        stLabel = QLabel("Score Thresh:")
        stHelp = make_help_label("Minimum confidence score for a detection (0 to 1).")
        self.sthrSpin = QDoubleSpinBox()
        self.sthrSpin.setRange(0, 1)
        self.sthrSpin.setValue(0.9)
        stRow = QHBoxLayout()
        stRow.setSpacing(0)
        stRow.setContentsMargins(0, 0, 0, 0)
        stRow.addWidget(self.sthrSpin)
        stRow.addWidget(stHelp)
        stContainer = QWidget()
        stContainer.setLayout(stRow)
        layout.addWidget(stLabel, 2, 0)
        layout.addWidget(stContainer, 2, 1)
        self.setLayout(layout)


class InferenceBox(QGroupBox):
    """
    Group box for performing model inference.
    """
    spectrogramSignal = pyqtSignal(object, list, tuple)
    logSignal = pyqtSignal(str)
    detectionSignal = pyqtSignal(dict)
    inferenceSpeedSignal = pyqtSignal(float)
    finishedSignal = pyqtSignal(bool)

    def __init__(self, parent=None):
        super().__init__("Inference", parent)
        self.setStyleSheet(
            "QGroupBox { margin-top: 20px; padding: 0px 10px; "
            "border: 1px solid #999; }"
        )
        layout = QVBoxLayout(self)
        layout.setSpacing(0)
        layout.setContentsMargins(0, 0, 0, 0)
        self.setLayout(layout)
        self.resourcePanel = None  # Will be set by MainWindow

        self.tabs = QTabWidget()
        self.tabs.setStyleSheet(
            "QTabWidget::pane { margin: 4px; padding: 4px; }"
            "QTabBar::tab { margin: 2px; padding: 6px; border: 1px solid gray; "
            "border-radius: 3px; }"
            "QTabBar::tab:selected { border: 2px solid blue; }"
        )
        layout.addWidget(self.tabs)

        self.offlineTab = OfflineWidget()
        self.tabs.addTab(self.offlineTab, "Offline")
        self.liveTab = LiveWidget()
        self.tabs.addTab(self.liveTab, "Live")

        form_bottom = QFormLayout()
        form_bottom.setSpacing(0)
        form_bottom.setContentsMargins(0, 0, 0, 0)

        # Pre-load model path for speed
        default_model = os.path.join(MODELS_DIR, "curated", "modern_burst_gap_fold3.pth")
        self.modelPathEdit = QLineEdit(default_model)
        self.modelPathEdit.setMinimumWidth(250)
        self.modelPathEdit.setToolTip(self.modelPathEdit.text())
        self.modelPathEdit.textChanged.connect(lambda t: self.modelPathEdit.setToolTip(t))
        browse_model = QPushButton("Browse Model")
        hb2 = QHBoxLayout()
        hb2.setSpacing(0)
        hb2.setContentsMargins(0, 0, 0, 0)
        hb2.addWidget(self.modelPathEdit)
        hb2.addWidget(browse_model)
        container = QWidget()
        container.setLayout(hb2)
        form_bottom.addRow("Model Path:", container)

        self.numClassSpin = QSpinBox()
        self.numClassSpin.setRange(1, 99)
        self.numClassSpin.setValue(2)
        form_bottom.addRow("Num Classes:", self.numClassSpin)

        self.singlePassChk = QCheckBox("Output Single-Pass")
        self.singlePassChk.setChecked(False)
        form_bottom.addRow("Single-Pass Output:", self.singlePassChk)

        # NEW: Precision Dropdown
        self.precisionCombo = QComboBox()
        self.precisionCombo.addItems(["fp32", "fp16", "int8"])
        self.precisionCombo.setCurrentText("fp32")
        form_bottom.addRow("Precision:", self.precisionCombo)

        layout.addLayout(form_bottom)

        # Control buttons row
        hb3 = QHBoxLayout()
        hb3.setSpacing(0)
        hb3.setContentsMargins(0, 0, 0, 0)
        self.startBtn = QPushButton("Start")
        self.pauseBtn = QPushButton("Pause")
        self.resumeBtn = QPushButton("Resume")
        self.stopBtn = QPushButton("Stop")
        
        # Initially disabled
        self.pauseBtn.setEnabled(False)
        self.resumeBtn.setEnabled(False)
        self.stopBtn.setEnabled(False)
        
        hb3.addWidget(self.startBtn)
        hb3.addWidget(self.pauseBtn)
        hb3.addWidget(self.resumeBtn)
        hb3.addWidget(self.stopBtn)
        layout.addLayout(hb3)

        # Speed control row
        speed_row = QHBoxLayout()
        speed_row.setSpacing(5)
        speed_row.setContentsMargins(0, 0, 0, 0)
        
        speed_label = QLabel("Display Speed:")
        self.speedSlider = QSpinBox()
        self.speedSlider.setRange(1, 100)  # 1% to 100% speed
        self.speedSlider.setValue(100)  # Default to full speed
        self.speedSlider.setSuffix("%")
        self.speedSlider.setToolTip("Control display speed: 1% = very slow, 100% = full speed")
        
        speed_row.addWidget(speed_label)
        speed_row.addWidget(self.speedSlider)
        speed_row.addStretch()
        layout.addLayout(speed_row)

        self.thread = None
        self.worker = None
        self.startBtn.clicked.connect(self.startInference)
        self.pauseBtn.clicked.connect(self.pauseInference)
        self.resumeBtn.clicked.connect(self.resumeInference)
        self.stopBtn.clicked.connect(self.stopInference)
        self.speedSlider.valueChanged.connect(self.updateSpeed)
        browse_model.clicked.connect(self.browseModel)

    def browseModel(self):
        fn, _ = QFileDialog.getOpenFileName(
            self,
            "Select Model",
            os.getcwd(),
            "*.pth"
        )
        if fn:
            self.modelPathEdit.setText(fn)

    def startInference(self):
        self.startBtn.setEnabled(False)
        self.pauseBtn.setEnabled(True)
        self.resumeBtn.setEnabled(False)
        self.stopBtn.setEnabled(True)
        idx = self.tabs.currentIndex()
        model_path = self.modelPathEdit.text().strip()
        num_classes = self.numClassSpin.value()
        precision = self.precisionCombo.currentText()

        if idx == 0:
            # Offline
            fp = self.offlineTab.filePathEdit.text().strip()
            if not os.path.isfile(fp):
                self.logSignal.emit("ERROR: No valid IQ file found for offline mode.")
                self.startBtn.setEnabled(True)
                self.stopBtn.setEnabled(False)
                return
            sr = self.offlineTab.offlineSampleRate
            bw = self.offlineTab.offlineBandwidth
            sf = self.offlineTab.offlineSignalFormat
            # Debug print to see the values
            self.logSignal.emit(f"[DEBUG] Sample Rate: {sr}, Bandwidth: {bw}, Format: {sf}")
            if sr is None or bw is None or sf is None:
                self.logSignal.emit("ERROR: File properties were not set. Please select an IQ file.")
                self.startBtn.setEnabled(True)
                self.stopBtn.setEnabled(False)
                return
            params = {
                "offline_mode": True,
                "file_path": fp,
                "sample_rate": sr,
                "bandwidth": bw,
                "signal_format": sf,
                "score_thresh": self.offlineTab.sthrSpin.value(),
                "freq": 0.0,
                "gain": 0.0,
                "model_path": model_path,
                "num_classes": num_classes,
                "single_pass": self.singlePassChk.isChecked(),
                "precision": precision
            }
        else:
            # Live
            params = {
                "offline_mode": False,
                "file_path": "",
                "signal_format": "",
                "sample_rate": self.liveTab.srSpin.value(),
                "freq": self.liveTab.freqSpin.value(),
                "gain": self.liveTab.gainSpin.value(),
                "bandwidth": self.liveTab.bwSpin.value(),
                "score_thresh": self.liveTab.sthrSpin.value(),
                "model_path": model_path,
                "num_classes": num_classes,
                "single_pass": False,
                "precision": precision
            }

        self.worker = InferenceWorker(params)
        self.thread = QThread()
        self.worker.moveToThread(self.thread)
        self.worker.progress.connect(self.logSignal.emit)
        self.worker.finished.connect(self.onFinished)
        self.worker.spectrogramReady.connect(self.spectrogramSignal.emit)
        self.worker.detectionJson.connect(self.detectionSignal.emit)
        self.worker.inferenceSpeedUpdated.connect(self.inferenceSpeedSignal.emit)
        self.worker.throughputUpdated.connect(self.onThroughputUpdate)
        self.thread.started.connect(self.worker.run)
        self.thread.start()
    
    def onThroughputUpdate(self, throughput):
        """Handle throughput updates from worker"""
        # Debug log
        self.logSignal.emit(f"[DEBUG] Received throughput: {throughput:.2f} chunks/sec")
        # Use direct reference to resource panel
        if self.resourcePanel:
            self.resourcePanel.setThroughput(throughput)
        else:
            self.logSignal.emit("[ERROR] ResourcePanel not set!")

    def pauseInference(self):
        if self.worker:
            self.worker.pause()
        # Also pause the display buffer
        if hasattr(self, 'mainWindow') and self.mainWindow:
            self.mainWindow.pauseDisplay()
        self.pauseBtn.setEnabled(False)
        self.resumeBtn.setEnabled(True)
        self.logSignal.emit("[INFO] Inference paused")

    def resumeInference(self):
        if self.worker:
            self.worker.resume()
        # Also resume the display buffer
        if hasattr(self, 'mainWindow') and self.mainWindow:
            self.mainWindow.resumeDisplay()
        self.pauseBtn.setEnabled(True)
        self.resumeBtn.setEnabled(False)
        self.logSignal.emit("[INFO] Inference resumed")

    def updateSpeed(self, speed_percent):
        if self.worker:
            self.worker.setDisplaySpeed(speed_percent)
        # Also update the display buffer speed
        if hasattr(self, 'mainWindow') and self.mainWindow:
            self.mainWindow.updateDisplaySpeed(speed_percent)
        self.logSignal.emit(f"[INFO] Display speed set to {speed_percent}%")

    def stopInference(self):
        if self.worker:
            self.worker.stop()

    def onFinished(self, success):
        self.startBtn.setEnabled(True)
        self.pauseBtn.setEnabled(False)
        self.resumeBtn.setEnabled(False)
        self.stopBtn.setEnabled(False)
        if self.thread:
            self.thread.quit()
            self.thread.wait()
        self.finishedSignal.emit(success)


class MainWindow(QMainWindow):
    """
    Main application window for TENSORCADE.
    """
    def __init__(self):
        super().__init__()
        self.setWindowTitle("TENSORCADE")
        self.resize(1600, 900)
        menubar = self.menuBar()
        menu = menubar.addMenu("Menu")
        openConfigAction = QAction("Open Config", self)
        openConfigAction.triggered.connect(self.showMasterConfig)
        menu.addAction(openConfigAction)
        central = QWidget()
        self.setCentralWidget(central)
        mainLayout = QHBoxLayout(central)
        mainLayout.setSpacing(2)
        mainLayout.setContentsMargins(2, 2, 2, 2)
        leftCol = QVBoxLayout()
        leftCol.setSpacing(2)
        leftCol.setContentsMargins(2, 2, 2, 2)
        leftColWidget = QWidget()
        leftColWidget.setLayout(leftCol)
        leftColWidget.setMaximumWidth(450)
        topRow = QHBoxLayout()
        topRow.setSpacing(5)
        self.resourcePanel = ResourceUsagePanel()
        topRow.addWidget(self.resourcePanel, stretch=0)
        topRow.addStretch(1)
        leftCol.addLayout(topRow)
        self.createBox = CreateImagesBox()
        self.createBox.logSignal.connect(self.appendLog)
        leftCol.addWidget(self.createBox)
        self.trainBox = TrainModelBox()
        self.trainBox.logSignal.connect(self.appendLog)
        leftCol.addWidget(self.trainBox)
        self.inferBox = InferenceBox()
        self.inferBox.logSignal.connect(self.appendLog)
        self.inferBox.spectrogramSignal.connect(self.receiveSpectrogram)
        self.inferBox.detectionSignal.connect(self.handleDetection)
        self.inferBox.inferenceSpeedSignal.connect(self.resourcePanel.setInferenceSpeed)
        # Set the resource panel reference for throughput updates
        self.inferBox.resourcePanel = self.resourcePanel
        # Set the main window reference for display control
        self.inferBox.mainWindow = self
        leftCol.addWidget(self.inferBox)
        self.logEdit = QPlainTextEdit()
        self.logEdit.setReadOnly(True)
        leftCol.addWidget(self.logEdit, stretch=1)
        mainLayout.addWidget(leftColWidget, stretch=0)
        # PyQtGraph setup for fast display
        self.canvas = pg.GraphicsLayoutWidget()
        self.canvas.setSizePolicy(QSizePolicy.Expanding, QSizePolicy.Expanding)
        mainLayout.addWidget(self.canvas, stretch=1)
        self.plot = self.canvas.addPlot()
        self.plot.setLabel('bottom', 'Time', 's')
        self.plot.setLabel('left', 'Frequency', 'Hz')
        self.img_item = pg.ImageItem()
        self.plot.addItem(self.img_item)
        # Set up grayscale colormap
        colors = [(i, i, i) for i in range(256)]
        cmap = pg.ColorMap(pos=np.linspace(0.0, 1.0, 256), color=colors)
        self.img_item.setLookupTable(cmap.getLookupTable(0.0, 1.0, 256))
        self.box_items = []
        self.text_items = []
        # Limit max boxes to prevent memory growth
        self.max_boxes = 100
        
        # Display buffer system for smooth speed control
        self.display_buffer = deque(maxlen=100)  # Buffer to store incoming spectrograms
        self.display_timer = QTimer()
        self.display_timer.timeout.connect(self.processDisplayBuffer)
        self.display_rate_ms = 100  # Default 100ms between frames (10 fps)
        self.display_paused = False
        
        # Start the display timer
        self.display_timer.start(self.display_rate_ms)

    def showMasterConfig(self):
        dlg = MasterConfigDialog(self)
        dlg.setModal(False)
        dlg.show()

    def appendLog(self, msg):
        self.logEdit.appendPlainText(msg)
        self.logEdit.moveCursor(QTextCursor.End)

    # ===== PyQtGraph updateSpectrogram Method =====
    def updateSpectrogram(self, spect2d, boxes, extent):
        """
        Update the spectrogram display with new image and boxes using PyQtGraph.
        
        extent = [x_min, x_max, y_min, y_max]
        """
        # Set image with levels for float data
        self.img_item.setImage(spect2d.T, autoLevels=False, levels=[0, 1])
        self.img_item.setRect(pg.QtCore.QRectF(extent[0], extent[2], 
                                                extent[1] - extent[0], 
                                                extent[3] - extent[2]))
        
        # Calculate scaling
        out_size = app_settings["out_size"]
        x_scale = (extent[1] - extent[0]) / out_size
        y_scale = (extent[3] - extent[2]) / out_size
        
        # Remove old boxes and text with proper cleanup
        for item in self.box_items:
            self.plot.removeItem(item)
            item.deleteLater()  # Ensure proper cleanup
        for item in self.text_items:
            self.plot.removeItem(item) 
            item.deleteLater()  # Ensure proper cleanup
        self.box_items.clear()
        self.text_items.clear()
        
        # Limit number of boxes to prevent slowdown
        if len(boxes) > self.max_boxes:
            boxes = boxes[:self.max_boxes]
        
        # Add new boxes
        pen = pg.mkPen('r', width=2)
        for (x1, y1, x2, y2, score) in boxes:
            # Convert pixel coordinates to data coordinates
            x1_new = x1 * x_scale + extent[0]
            x2_new = x2 * x_scale + extent[0]
            y1_new = y1 * y_scale + extent[2]
            y2_new = y2 * y_scale + extent[2]
            
            # Create rectangle
            rect = pg.RectROI([x1_new, y1_new], 
                             [x2_new - x1_new, y2_new - y1_new], 
                             pen=pen, movable=False, removable=False)
            self.plot.addItem(rect)
            self.box_items.append(rect)
            
            # Add score text
            text = pg.TextItem(f'{score:.2f}', color='yellow', anchor=(0, 1))
            text.setPos(x1_new, y2_new)
            self.plot.addItem(text)
            self.text_items.append(text)
        
        # Update plot title
        self.plot.setTitle("Inference Spectrogram + Boxes")

    def receiveSpectrogram(self, spect2d, boxes, extent):
        """
        Receive incoming spectrogram data and add to display buffer.
        This runs at full processing speed.
        """
        if not self.display_paused:
            # Add to buffer (automatically removes oldest if buffer is full)
            self.display_buffer.append((spect2d, boxes, extent))

    def processDisplayBuffer(self):
        """
        Timer-driven method to pull from buffer and display at controlled rate.
        """
        if not self.display_paused and self.display_buffer:
            # Get the most recent spectrogram from buffer
            spect2d, boxes, extent = self.display_buffer.popleft()
            self.updateSpectrogram(spect2d, boxes, extent)

    def updateDisplaySpeed(self, speed_percent):
        """
        Update display timer interval based on speed percentage.
        speed_percent: 1-100 (1% = very slow, 100% = full speed)
        """
        if speed_percent < 1:
            speed_percent = 1
        elif speed_percent > 100:
            speed_percent = 100
            
        # Calculate timer interval: 100ms base rate adjusted by speed
        # At 100% speed: 100ms interval (10 fps)
        # At 50% speed: 200ms interval (5 fps)  
        # At 1% speed: 10000ms interval (0.1 fps)
        self.display_rate_ms = int(100 * (100 / speed_percent))
        
        # Restart timer with new interval
        self.display_timer.stop()
        self.display_timer.start(self.display_rate_ms)
        
        self.appendLog(f"[INFO] Display rate updated to {self.display_rate_ms}ms ({speed_percent}% speed)")

    def pauseDisplay(self):
        """
        Pause the display buffer processing.
        """
        self.display_paused = True
        self.appendLog("[INFO] Display paused")

    def resumeDisplay(self):
        """
        Resume the display buffer processing.
        """
        self.display_paused = False  
        self.appendLog("[INFO] Display resumed")

    def handleDetection(self, detection_dict):
        import json
        txt = json.dumps(detection_dict, indent=2)
        self.appendLog(txt)


================================================================================
# FILE: ARCADE/src/tensorcade/main.py
================================================================================
"""
TENSORCADE Main Entry Point

This module serves as the main entry point for the TENSORCADE application,
a PyQt5-based GUI tool for tensor-based signal detection and analysis.

TENSORCADE provides capabilities for:
    - Machine learning-based signal detection
    - Training custom detection models
    - Real-time and offline inference on IQ data
    - Spectrogram generation and visualization
    - Resource monitoring (CPU, RAM, GPU)

Key Features:
    - Telemetry tracking for performance analysis
    - Process ID management for multi-instance handling
    - Automatic cleanup on application exit
    
Usage:
    Run as a module: python -m tensorcade
    Or directly: python main.py

Environment Variables:
    TENSORCADE_MAIN_PID: Set to current process ID for tracking
"""

import os, sys
# Set process ID in environment for telemetry and multi-instance tracking
os.environ["TENSORCADE_MAIN_PID"] = str(os.getpid())

import tensorcade.telemetry as tele
from PyQt5.QtWidgets import QApplication
from .widgets import MainWindow

def main():
    """
    Initialize and launch the TENSORCADE application.
    
    Creates the PyQt5 application instance, sets up telemetry cleanup
    handlers, initializes the main window, and starts the Qt event loop.
    
    The telemetry cleanup handler ensures proper resource cleanup when
    the application exits, preventing resource leaks and ensuring clean
    shutdown of monitoring threads.
    
    Returns:
        int: Application exit code (0 for normal exit, non-zero for errors)
    """
    app = QApplication(sys.argv)
    # Register cleanup handler to purge telemetry data on GUI close
    app.aboutToQuit.connect(tele._cleanup)
    win = MainWindow()
    win.show()
    sys.exit(app.exec_())

if __name__ == "__main__":
    main()


================================================================================
# FILE: ARCADE/src/tensorcade/cli_headless.py
================================================================================
import os, sys, time, gc, tracemalloc, argparse, signal, atexit
os.environ["TENSORCADE_MAIN_PID"] = str(os.getpid())

from pathlib import Path
from PyQt5.QtCore import QCoreApplication, QThread
from tensorcade.workers   import InferenceWorker
from tensorcade.telemetry import _cleanup
from tensorcade.config    import app_settings


def _build_arg_parser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser(prog="tensorcade-headless")
    p.add_argument("-m", "--model",        required=True, type=Path)
    p.add_argument("-i", "--iq",           required=True, type=Path)
    p.add_argument("-s", "--sample-rate",  required=True, type=float)
    p.add_argument("-b", "--bandwidth",    required=True, type=float)
    p.add_argument("-n", "--num-classes",  default=2, type=int)
    p.add_argument("--fp16",     action="store_true")
    p.add_argument("--iterations", default=300, type=int)
    return p


def main(argv=None) -> None:
    a = _build_arg_parser().parse_args(argv)
    params = {
        "offline_mode": True,
        "file_path":    str(a.iq),
        "sample_rate":  a.sample_rate,
        "bandwidth":    a.bandwidth,
        "signal_format": "",
        "score_thresh": .5,
        "freq": 0.0,
        "gain": 0.0,
        "model_path":   str(a.model),
        "num_classes":  a.num_classes,
        "single_pass":  False,
        "precision":    "fp16" if a.fp16 else "fp32",
    }

    app  = QCoreApplication([])
    thr  = QThread()
    work = InferenceWorker(params)
    work.moveToThread(thr)

    stats = {"chunks": 0, "t0": time.time()}

    work.progress.connect(lambda m: print(m, flush=True))
    work.inferenceSpeedUpdated.connect(lambda _: stats.__setitem__("chunks", stats["chunks"] + 1))
    work.finished.connect(app.quit)
    thr.started.connect(work.run)

    def _stop_all():
        work.stop()
        thr.quit()
        _cleanup()

    def _sigint(_sig, _frm):
        print("\n[CTRL-C] shutting down …", flush=True)
        _stop_all()
        app.quit()

    signal.signal(signal.SIGINT, _sigint)
    atexit.register(_stop_all)

    tracemalloc.start()
    thr.start()
    app.exec_()
    thr.wait()
    cur, peak = tracemalloc.get_traced_memory()
    tracemalloc.stop()

    dur = time.time() - stats["t0"]
    print(f"\nProcessed {stats['chunks']} chunks in {dur:0.1f}s")
    print(f"Current RSS tracked by tracemalloc: {cur/1_048_576:0.2f} MiB")
    print(f"Peak   RSS tracked by tracemalloc: {peak/1_048_576:0.2f} MiB")
    gc.collect()


if __name__ == "__main__":
    main()



================================================================================
# FILE: ARCADE/src/tensorcade/readme.md
================================================================================
# TENSORCADE

TENSORCADE is a spectrogram analysis tool that:
- **Processes IQ Data:** Reads IQ files (e.g., `.sigmf-data`) and generates spectrogram images.
- **Trains Detection Models:** Uses generated spectrograms to train a signal detection model with PyTorch.
- **Performs Inference:** Runs offline or live inference on IQ data to detect and label signals.
- **Provides a Graphical Interface:** Built with PyQt5, it offers a single-page UI for configuration, model training, and inference.

> **Note:** To leverage GPU acceleration, users **must have CUDA 11.8 installed** and the CUDA-enabled PyTorch wheels will be installed automatically.

## Installation

TENSORCADE is packaged to be installed via `pip` and made globally available using [pipx](https://pipxproject.github.io/pipx/). The package is configured to automatically install the CUDA-enabled versions of PyTorch and torchvision for Python 3.12 on Linux with CUDA 11.8.

### Prerequisites

- **CUDA 11.8:** Ensure your system has CUDA 11.8 installed along with the appropriate NVIDIA drivers.
- **Python 3.12:** TENSORCADE is built for Python 3.12.
- **pipx:** (Optional, but recommended) Install pipx so that you can install TENSORCADE in an isolated environment.

To install pipx:
```bash
python3 -m pip install --user pipx
python3 -m pipx ensurepath

