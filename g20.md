# G20 System Documentation

**Last Updated:** 2026-02-01
**Purpose:** Feature reference with key assumptions - prevents context loss and mistakes

---

## Architecture Overview

```
Flutter App ←─ WebSocket ─→ Python Backend
     │                           │
     ├── Training Screen         ├── server.py (WebSocket router)
     ├── Live Detection          ├── unified_pipeline.py (FFT + inference)
     └── Settings                └── training/ (sample_manager, service)
                                 └── hydra/ (detector, version_manager)
```

---

## 1. Backend Launcher (`lib/core/services/backend_launcher.dart`)

**What it does:** Auto-starts Python backend on app launch, tracks via PID file, ensures cleanup on close.

**Key assumptions:**
- Backend lives at `backend/server.py` relative to Flutter working directory
- Python server writes `runtime/server.json` when ready (contains WS port)
- Uses `--ws-port 0` for OS-assigned port (avoids conflicts)
- Window close triggers `taskkill /F /T` on Windows (kills entire process tree)

**Startup flow:**
1. Kill any stale PID from `.backend.pid`
2. Delete `runtime/server.json` (clean slate)
3. Start `python server.py --port 50051 --ws-port 0`
4. Poll for `server.json` (30s timeout)
5. Read `ws_port` from JSON, connect

**Shutdown flow:**
1. `window_manager` intercepts close
2. Calls `stopBackend()`
3. `taskkill /F /T /PID` (Windows) or SIGTERM→SIGKILL (Unix)
4. Delete PID file

---

## 2. RFCAP File Format (`lib/core/services/rfcap_service.dart`)

**What it does:** Binary file format for IQ captures.

**Format:**
```
Offset  Size   Type      Field
0       4      char[4]   Magic "G20\0"
4       4      uint32    Version (1)
8       8      float64   Sample rate (Hz)
16      8      float64   Center freq (Hz)
24      8      float64   Bandwidth (Hz)
32      8      uint64    Number of samples
40      8      float64   Start time (epoch)
48      32     char[32]  Signal name (null-padded)
80      8      float64   Latitude
88      8      float64   Longitude
96      416    reserved  (zeros)
512+    N×8    complex64 IQ data (float32 I, float32 Q pairs)
```

**Key assumptions:**
- Header is ALWAYS 512 bytes
- IQ data is interleaved complex64 (8 bytes per sample)
- `bandwidth_hz` in header may differ from FFT bandwidth (FFT uses sample_rate as bandwidth)

---

## 3. Training System

### 3.1 Training Presets (`backend/hydra/config.py`)

| Preset   | Epochs | LR     | Batch | Patience | Warmup | Use Case |
|----------|--------|--------|-------|----------|--------|----------|
| Fast     | 15     | 0.001  | 8     | 2        | 500    | Quick validation |
| Balanced | 30     | 0.001  | 4     | 5        | 1000   | Production default |
| Quality  | 75     | 0.001  | 4     | 10       | 1500   | Maximum accuracy |
| Extreme  | 150    | 0.0005 | 2     | 20       | 2000   | Difficult signals |

**Key assumptions:**
- Flutter sends `training_config` object with all 5 values
- Backend must extract and pass ALL values to `TrainingService.train_new_signal()` / `extend_signal()`
- **BUG FIXED:** Previously backend ignored training_config and used defaults

### 3.2 Training Provider (`lib/features/training/providers/training_provider.dart`)

**Flow:**
1. User draws boxes on spectrogram (in real-world units: seconds, MHz)
2. User selects preset (Fast/Balanced/Quality/Extreme)
3. Flutter sends `train_signal` via WebSocket with:
   - `signal_name`
   - `training_config: {epochs, learning_rate, batch_size, early_stop_patience, warmup_iterations}`
   - IQ data and boxes
4. Backend saves samples, runs training, sends progress updates

### 3.3 Sample Manager (`backend/training/sample_manager.py`)

**What it does:** Converts Flutter labels (real-world coords) to Python spectrograms.

**CRITICAL COORDINATE FIX (Jan 2026):**
- Flutter sends REAL UNITS: `time_start_sec`, `time_end_sec`, `freq_start_mhz`, `freq_end_mhz`
- Python extracts 0.1s IQ window CENTERED on box
- Python computes its OWN spectrogram (4096 FFT, 2048 hop, 80dB)
- Real units → pixel coords for Python's spectrogram (NOT Flutter's)

**Why this matters:**
- Flutter's spectrogram differs from inference spectrogram (different FFT params)
- Using real units as source of truth ensures training data matches inference

**Assumptions:**
- `TRAINING_WINDOW_SEC = 0.1` (fixed 100ms window per sample)
- FFT bandwidth = sample_rate (Nyquist), NOT header bandwidth field
- Y=0 is HIGH frequency after `flipud()`

### 3.4 Version Manager (`backend/hydra/version_manager.py`)

**What it does:** Manages model versions with auto-promotion.

**Auto-promotion logic:**
- F1 ≥ 0.95: Promote if new ≥ old (no regression allowed)
- F1 < 0.95: Require +2% absolute improvement

**Version retention:** Keep last 5 versions, never delete active.

**Files:**
- `models/heads/{signal_name}/v{N}.pth` - version weights
- `models/heads/{signal_name}/active.pth` - symlink/copy of active version
- `models/heads/{signal_name}/metadata.json` - version history
- `models/registry.json` - central index of all signals

---

## 4. Live Detection System

### 4.1 Unified Pipeline (`backend/unified_pipeline.py`)

**What it does:** Processes IQ → FFT → RGBA pixels + inference.

**DUAL FFT PATHS (CRITICAL):**
| Path       | FFT Size | Hop    | Dynamic Range | Purpose |
|------------|----------|--------|---------------|---------|
| Inference  | 4096     | 2048   | 80 dB         | MUST match model training (TensorCade) |
| Waterfall  | 8K-64K   | 50%    | 60 dB         | User-adjustable display |

**Row-strip streaming:**
- Backend sends ~20 RGBA row strips per frame (fixed via max-pooling decimation)
- Flutter stitches strips into scrolling waterfall buffer
- Detection boxes tracked by absolute row index

**Protocol:**
- `0x01` + header + RGBA + PSD: Row strip data
- `0x02` + JSON: Detection frame
- `0x03` + JSON: Metadata

**Header format (17 bytes):**
```
frame_id:     uint32 (4)
total_rows:   uint32 (4) - monotonic counter
rows_in_strip: uint16 (2)
strip_width:  uint16 (2)
pts:          float32 (4)
source_id:    uint8 (1) - 0=SCAN, 1=RX1_REC, 2=RX2_REC, 3=MANUAL
```

### 4.2 Hydra Detector (`backend/hydra/detector.py`)

**What it does:** Multi-head detection with shared Faster R-CNN backbone.

**Architecture:**
- Shared ResNet50 backbone (frozen during fine-tuning)
- Per-signal detection heads (loaded dynamically)
- Heads registered in `models/registry.json`

**Commands via WebSocket:**
- `load_heads`: Load specific signal heads for mission
- `unload_heads`: Free memory by unloading unused heads
- `get_available_signals`: List all trained signals
- `get_loaded_heads`: List currently active heads

**Assumptions:**
- Backbone loaded at startup (no heads)
- Heads loaded when mission starts via `load_heads` command
- Inference skipped with warning if no heads loaded

### 4.3 GPU FFT Processor (`backend/gpu_fft.py`)

**What it does:** CUDA-accelerated FFT via cuFFT.

**Key features:**
- Batched FFT processing (5-10x faster than CPU numpy)
- Pre-compiled cuFFT plans (warmup on size change: 100-500ms)
- Always outputs ~20 rows regardless of FFT size (max-pooling decimation)

**Valid FFT sizes:** 8192, 16384, 32768, 65536

---

## 5. WebSocket Commands Reference

### Training (`/training` endpoint)

| Command | Params | Response |
|---------|--------|----------|
| `train_signal` | `signal_name`, `training_config: {epochs, learning_rate, batch_size, early_stop_patience, warmup_iterations}`, `is_new` | `training_progress`, then `training_complete` |
| `cancel_training` | - | `training_cancelled` |
| `save_sample` | `signal_name`, `iq_data` (base64), `boxes`, `metadata` | `sample_saved` |
| `get_registry` | - | `registry` |
| `promote_version` | `signal_name`, `version` | `version_promoted` |
| `rollback_signal` | `signal_name` | `version_rollback` |

### Video (`/video` endpoint)

| Command | Params | Response |
|---------|--------|----------|
| `set_fft_size` | `size` (8192/16384/32768/65536) | `fft_size_ack` |
| `set_time_span` | `seconds` | `time_span_ack` |
| `set_fps` | `fps` (1-60) | `fps_ack` |
| `set_score_threshold` | `threshold` (0.0-1.0) | `score_threshold_ack` |
| `set_colormap` | `colormap` (0-4) | `colormap_ack` |
| `load_heads` | `signal_names: [...]` | `heads_loaded` |
| `unload_heads` | `signal_names` (optional) | `heads_unloaded` |

---

## 6. Known Gotchas

1. **Training config must be explicitly extracted** - Server must parse `training_config` dict and pass each value individually to training service.

2. **FFT bandwidth ≠ header bandwidth** - For coordinate conversion, use `sample_rate` as bandwidth (FFT spans -fs/2 to +fs/2).

3. **Y=0 is HIGH frequency** - After `flipud()`, spectrogram has high freq at top.

4. **Warmup on FFT size change** - cuFFT plan compilation takes 100-500ms; don't rapid-fire changes.

5. **No heads = no inference** - Hydra requires explicit `load_heads` command before detection works.

6. **Window close cleanup** - Uses `window_manager` for reliable cleanup on desktop; `didChangeAppLifecycleState` is backup.

---

## 7. Crop Classifier (New - Week 1 Complete)

### 7.1 Overview

Two-stage detection system replacing position-biased Faster R-CNN:
1. **Stage 1:** Classical blob detection (Otsu + adaptive threshold) - no position bias
2. **Stage 2:** Crop classifier (Siamese or CNN) - sees only cropped signal, never position

**Target:** 25 manual label decisions → 200+ training samples via confirmation UI

### 7.2 Module Structure

```
backend/crop_classifier/
├── __init__.py
├── models/
│   ├── siamese.py         # SiameseEncoder, SiameseNetwork, SiameseClassifier
│   └── losses.py          # ContrastiveLoss, FocalLoss, TripletLoss
├── inference/
│   ├── blob_detector.py   # BlobDetector with locked config
│   └── preprocessor.py    # CropPreprocessor, 32×64 letterbox
├── training/              # Placeholder for Week 3
└── labeling/              # Placeholder for Week 3

config/
└── crop_classifier.yaml   # Full configuration
```

### 7.3 Locked Blob Detection Config

**Do not change without re-running validation:**

```python
min_area = 50
max_area = 5000
min_aspect_ratio = 1.5
max_aspect_ratio = 15.0
block_size = 51
C = -5
```

Validated at 91.7% recall on Creamy_Shrimp dataset (48 samples).

### 7.4 Key Classes

| Class | Purpose |
|-------|---------|
| `BlobDetector` | Classical blob detection with locked config |
| `CropPreprocessor` | Extract crops, letterbox to 32×64, per-crop normalize |
| `SiameseEncoder` | 3-layer CNN → 64-dim L2-normalized embedding |
| `SiameseNetwork` | Full Siamese for similarity learning |
| `SiameseClassifier` | Compare crops against gallery for classification |
| `ContrastiveLoss` | Training loss for Siamese networks |
| `FocalLoss` | Training loss for direct CNN (100+ labels) |

### 7.5 Crop Preprocessing

- **Target size:** 32×64 (H×W) - wider than tall for RF signals
- **Padding:** 15% around detected bounding boxes
- **Normalization:** Per-crop (mean=0, std=1)
- **Letterbox:** Aspect-preserving resize with zero-padding

### 7.6 Implementation Status

| Week | Status | Description |
|------|--------|-------------|
| Pre-work | ✅ COMPLETE | Blob validation (91.7% recall) |
| Week 1 | ✅ COMPLETE | Foundation: blob detector, preprocessor, Siamese |
| Week 2 | ⏳ TODO | Flutter labeling UI |
| Week 3 | ⏳ TODO | Training pipeline, active learning |
| Week 4 | ⏳ TODO | Direct CNN, augmentation |
| Week 5 | ⏳ TODO | Inference pipeline |
| Week 6 | ⏳ TODO | Production hardening |

---

## 8. Directory Structure

```
g20_demo/
├── backend/
│   ├── server.py              # WebSocket + gRPC server
│   ├── unified_pipeline.py    # FFT + inference pipeline
│   ├── gpu_fft.py             # CUDA FFT processor
│   ├── inference.py           # Legacy inference engine
│   ├── hydra/
│   │   ├── config.py          # Training presets, FFT params
│   │   ├── detector.py        # Multi-head Faster R-CNN
│   │   └── version_manager.py # Model versioning
│   └── training/
│       ├── sample_manager.py  # Real-world coord → spectrogram
│       ├── service.py         # Training orchestration
│       └── splits.py          # Train/val split management
├── lib/
│   ├── core/
│   │   └── services/
│   │       ├── backend_launcher.dart  # Auto-start Python
│   │       └── rfcap_service.dart     # IQ file format
│   └── features/
│       ├── training/
│       │   ├── training_screen.dart
│       │   └── providers/training_provider.dart
│       └── live_detection/
│           └── providers/video_stream_provider.dart
├── models/
│   ├── heads/{signal}/        # Per-signal detection heads
│   └── registry.json          # Central signal index
├── training_data/
│   └── signals/{signal}/      # Training samples per signal
├── config/
│   └── spectrogram.yaml       # Canonical FFT params
└── runtime/
    └── server.json            # Backend connection info
```

---

## Maintenance Notes

**When modifying training:**
- Update BOTH Flutter preset definitions AND Python `hydra/config.py`
- Ensure `train_signal` handler extracts ALL config fields

**When modifying FFT/spectrograms:**
- Inference params (4096/2048/80dB) are LOCKED to match model training
- Waterfall params can be user-adjusted

**When adding new signals:**
- Train via UI → creates `models/heads/{name}/`
- Registry auto-updates
- Mission must `load_heads` to enable detection
